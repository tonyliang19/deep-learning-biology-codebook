{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: Transformers\n",
        "\n",
        "Welcome to one of the most exciting chapters! \ud83d\ude80\n",
        "\n",
        "Transformers revolutionized AI starting in 2017. They power ChatGPT, Google Translate, protein structure prediction (AlphaFold), and much more. In this chapter, we'll demystify how they work.\n",
        "\n",
        "**What you'll learn:**\n",
        "- Why transformers are different from RNNs and CNNs\n",
        "- The \"attention mechanism\" - the key innovation (simpler than it sounds!)\n",
        "- How transformers process sequences (text, DNA, proteins)\n",
        "- Apply transformers to biological sequence analysis\n",
        "\n",
        "**Prerequisites:**\n",
        "- Chapter 1 (Neural Networks Basics)\n",
        "- Basic understanding of sequences (like sentences or DNA)\n",
        "- Comfort with matrix operations (we'll explain the concepts visually)\n",
        "\n",
        "**Don't be intimidated!** Transformers may sound complex, but the core idea is intuitive: \"pay attention to the important parts.\"\n",
        "\n",
        "## \ud83d\udcda Table of Contents\n",
        "1. [The Problem with Sequential Processing](#problem)\n",
        "2. [Attention Mechanism - The Key Idea](#attention)\n",
        "3. [Self-Attention Explained](#self-attention)\n",
        "4. [Multi-Head Attention](#multi-head)\n",
        "5. [Positional Encoding](#positional)\n",
        "6. [The Complete Transformer Architecture](#architecture)\n",
        "7. [BERT and GPT Overview](#models)\n",
        "8. [Biology Application: Protein Sequence Analysis](#biology-app)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('\u2713 Libraries imported')\n",
        "print(f'PyTorch: {torch.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Introduction to Attention <a id=\"intro\"></a>\n\n### The Problem with RNNs\n\nTraditional RNNs process sequences sequentially:\n- **Slow**: Cannot parallelize\n- **Memory**: Struggle with long sequences\n- **Gradient**: Vanishing/exploding gradients\n\n### The Attention Solution\n\n**Key Idea**: Let the model learn which parts of the input to focus on.\n\n**Attention Formula**:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nwhere:\n- $Q$ = Query (what we're looking for)\n- $K$ = Key (what we have)\n- $V$ = Value (what we return)\n- $d_k$ = dimension of keys (for scaling)\n\n### Intuition\n\nThink of it like a dictionary lookup:\n1. **Query**: \"What am I looking for?\"\n2. **Key**: \"Does this match what you want?\"\n3. **Value**: \"Here's the information\"\n\nThe attention mechanism computes similarity between query and all keys, then returns a weighted sum of values.\n\n### Attention Mechanism: Connections to Classical Methods\n\nThe attention mechanism [@vaswani2017attention; @bahdanau2014neural] has deep connections to classical statistical and machine learning concepts.\n\n#### Attention as Weighted Regression\n\nAt its core, attention is performing **weighted regression** or **kernel smoothing** [@hastie2009elements]:\n\n$$\\text{Attention}(Q, K, V) = \\sum_{i} \\alpha_i V_i$$\n\nwhere the weights $\\alpha_i$ are computed based on the similarity between query $Q$ and keys $K_i$:\n\n$$\\alpha_i = \\frac{\\exp(Q \\cdot K_i / \\sqrt{d_k})}{\\sum_j \\exp(Q \\cdot K_j / \\sqrt{d_k})}$$\n\n**This is analogous to:**\n\n1. **Kernel Regression** [@scholkopf2002learning]: \n   - Predict output as weighted average of training outputs\n   - Weights based on kernel similarity $k(x, x_i)$\n   - Attention uses $\\exp(Q \\cdot K_i / \\sqrt{d_k})$ as the \"kernel\"\n\n2. **k-Nearest Neighbors (k-NN) with soft weights:**\n   - Instead of hard selection of k neighbors, attention uses soft weights\n   - All positions contribute, but closer ones (higher similarity) contribute more\n\n3. **Nadaraya-Watson Estimator** (non-parametric regression):\n   $$\\hat{f}(x) = \\frac{\\sum_i K(x, x_i) y_i}{\\sum_i K(x, x_i)}$$\n   - Replace kernel $K$ with attention weights \u2192 you get attention!\n\n#### Connection to Matrix Factorization and Low-Rank Approximation\n\nThe attention mechanism can be viewed as a form of **matrix factorization** [@koren2009matrix]:\n\nGiven input $X$ (sequence length $n$ \u00d7 embedding dimension $d$):\n- $Q = XW_Q$, $K = XW_K$, $V = XW_V$ (where $W$ matrices project to dimension $d_k$)\n- Attention computes: $\\text{softmax}(QK^T/\\sqrt{d_k}) V$\n\n**Key insight:** $QK^T$ is a **rank-$d_k$ approximation** to the full $n \\times n$ interaction matrix!\n\nIf we wanted to model all pairwise interactions directly:\n- We'd need an $n \\times n$ parameter matrix (quadratic in sequence length)\n- Instead, we factor it: $QK^T = (XW_Q)(XW_K)^T$ \n- This is a **low-rank factorization** with rank at most $d_k$ (much smaller than $n$ for long sequences)\n\n**Connection to classical methods:**\n- **Principal Component Analysis (PCA):** Low-rank approximation to covariance matrix\n- **Singular Value Decomposition (SVD):** Matrix factorization $A = U\\Sigma V^T$\n- **Attention:** Learned, query-dependent low-rank approximation to interaction matrix\n\n**Why this matters:** Like PCA/SVD, attention captures the \"most important\" interactions in a compressed form, but unlike classical methods, it's:\n- Learned end-to-end for the task\n- Query-dependent (changes based on what we're looking for)\n- Non-linear (through softmax and subsequent layers)\n\n#### Attention as an Associative Memory\n\nFrom a neuroscience perspective, attention implements an **associative memory** similar to Hopfield networks:\n\n- **Key-value storage:** Keys $K$ are like memory addresses, Values $V$ are stored content\n- **Retrieval:** Query $Q$ retrieves a weighted combination of values based on similarity to keys\n- **Soft retrieval:** Unlike exact memory lookup, attention performs soft, differentiable retrieval\n\nThis is analogous to **content-addressable memory** in computer science or **prototype-based models** in psychology and machine learning.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "    \n",
        "    Args:\n",
        "        Q: Query matrix (batch, seq_len, d_k)\n",
        "        K: Key matrix (batch, seq_len, d_k)\n",
        "        V: Value matrix (batch, seq_len, d_v)\n",
        "        mask: Optional mask\n",
        "    \n",
        "    Returns:\n",
        "        output: Attention output\n",
        "        attention_weights: Attention scores\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "    \n",
        "    # Compute attention scores\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    \n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    # Apply softmax\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    # Compute output\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Test with simple example\n",
        "print('Testing Attention Mechanism:')\n",
        "seq_len = 4\n",
        "d_model = 8\n",
        "\n",
        "Q = torch.randn(1, seq_len, d_model)\n",
        "K = torch.randn(1, seq_len, d_model)\n",
        "V = torch.randn(1, seq_len, d_model)\n",
        "\n",
        "output, weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(f'\\nInput shapes:')\n",
        "print(f'  Q: {Q.shape}')\n",
        "print(f'  K: {K.shape}')\n",
        "print(f'  V: {V.shape}')\n",
        "print(f'\\nOutput shapes:')\n",
        "print(f'  Output: {output.shape}')\n",
        "print(f'  Attention weights: {weights.shape}')\n",
        "\n",
        "# Visualize attention weights\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(weights[0].detach().numpy(), annot=True, fmt='.2f', \n",
        "            cmap='YlOrRd', cbar_kws={'label': 'Attention Weight'})\n",
        "plt.xlabel('Key Position', fontsize=12)\n",
        "plt.ylabel('Query Position', fontsize=12)\n",
        "plt.title('Attention Weight Matrix', fontsize=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n\ud83d\udca1 Each row shows where each query attends to!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Self-Attention Mechanism <a id=\"self-attention\"></a>\n\nIn **self-attention**, Q, K, and V all come from the same input!\n\n### Why Self-Attention?\n\nAllows each position to attend to all positions in the sequence:\n- Capture long-range dependencies\n- Parallel computation\n- No distance bias\n\n### How it Works\n\n1. Start with input embeddings $X$\n2. Create Q, K, V by linear transformations:\n   - $Q = XW_Q$\n   - $K = XW_K$\n   - $V = XW_V$\n3. Compute attention\n\n### Example: Protein Sequence\n\nFor sequence \"ACGT\":\n- A might attend strongly to C (if they interact)\n- G might attend to T (complementary bases)\n- Self-attention learns these relationships!\n\n### Self-Attention: Graph-Theoretic and Statistical Perspectives\n\n#### Self-Attention as Graph Neural Network\n\nSelf-attention can be viewed as a complete **graph neural network** where:\n- Nodes: Each position in the sequence\n- Edges: Attention weights between all pairs of positions\n- Message passing: Each node aggregates information from all other nodes, weighted by attention\n\n**Connection to graph theory:**\n- Self-attention computes on a **fully-connected graph**\n- Attention weights define edge strengths (learned, not fixed)\n- Similar to **spectral graph convolutions** but with learned adjacency matrix\n\n#### Relationship to Covariance and Correlation\n\nThe $QK^T$ matrix in self-attention computes pairwise similarities between all positions:\n\n$$(QK^T)_{ij} = q_i^T k_j = \\sum_{d=1}^{d_k} q_{i,d} k_{j,d}$$\n\nThis is similar to computing a **correlation matrix** in statistics, where we measure how related different variables (positions) are.\n\n**Classical covariance matrix:**\n$$\\Sigma_{ij} = \\text{Cov}(X_i, X_j) = \\mathbb{E}[(X_i - \\mu_i)(X_j - \\mu_j)]$$\n\n**Self-attention similarity:**\n$$A_{ij} = \\text{softmax}(q_i^T k_j / \\sqrt{d_k})$$\n\nBoth capture relationships between elements, but:\n- Covariance is computed from data statistics (second moments)\n- Attention is computed through learned projections (task-specific)\n- Attention uses softmax normalization (creates a probability distribution)\n\n#### Connection to Factor Analysis\n\nSelf-attention with multiple heads performs something similar to **factor analysis** [@bishop2006pattern]:\n\n**Factor analysis:** Assumes observed variables are generated from latent factors:\n$$X = WF + \\epsilon$$\n\n**Multi-head attention:** Different heads attend to different \"latent aspects\" of the sequence:\n$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n\nEach head can be thought of as discovering a different \"factor\" or \"aspect\" of the data:\n- Head 1: Local patterns (neighboring positions)\n- Head 2: Long-range dependencies  \n- Head 3: Specific semantic relationships\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-Attention layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Linear transformations for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Create Q, K, V\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "        \n",
        "        # Compute attention\n",
        "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Test\n",
        "d_model = 64\n",
        "seq_len = 5\n",
        "batch_size = 2\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "self_attn = SelfAttention(d_model)\n",
        "\n",
        "output, weights = self_attn(x)\n",
        "\n",
        "print('Self-Attention Layer:')\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Output shape: {output.shape}')\n",
        "print(f'Attention weights shape: {weights.shape}')\n",
        "print('\\n\u2713 Self-attention preserves sequence length!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Multi-Head Attention <a id=\"multi-head\"></a>\n\n### Why Multiple Heads?\n\nSingle attention focuses on one aspect. Multiple heads allow:\n- Different representation subspaces\n- Attend to different positions\n- Capture various relationships\n\n### Formula\n\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n\nwhere each head is:\n$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n\n### Analogy\n\nThink of multiple editors reviewing the same text:\n- One checks grammar\n- One checks style  \n- One checks content\n- Combined feedback is comprehensive!\n\n### Multi-Head Attention: Ensemble Methods and Subspace Learning\n\n#### Connection to Ensemble Learning\n\nMulti-head attention is conceptually similar to **ensemble methods** in machine learning [@hastie2009elements]:\n\n**Random Forests:** Combine multiple decision trees trained on different data subsets\n**Boosting:** Combine multiple weak learners with different weights\n**Multi-head Attention:** Combine multiple attention \"experts\" focusing on different representation subspaces\n\n**Key differences:**\n- Random forests: Trees operate independently on same features\n- Multi-head attention: Heads operate independently on different learned projections\n\nEach attention head learns a different **view** of the relationships in the sequence, analogous to:\n- Different features in feature selection\n- Different components in PCA\n- Different factors in factor analysis\n\n#### Subspace Learning and Dimensionality Reduction\n\nEach attention head operates in a lower-dimensional subspace:\n- Full embedding dimension: $d_{\\text{model}}$ (e.g., 512)\n- Each head dimension: $d_k = d_{\\text{model}} / h$ (e.g., 64 with 8 heads)\n\nThis is a form of **dimensionality reduction** similar to:\n\n**PCA projection:** Project data to top principal components\n**Random projection:** Project to random lower-dimensional subspace (Johnson-Lindenstrauss lemma)\n**Attention heads:** Project to *learned* task-specific subspaces\n\n**Why multiple subspaces help:**\n- In statistics, we know that data often lies on lower-dimensional manifolds\n- Different aspects of data may live in different subspaces\n- Multi-head attention automatically discovers these task-relevant subspaces\n\n#### Mathematical Formulation as Block-Structured Computation\n\nMulti-head attention can be written as:\n\n$$\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$$\n$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n\nThis creates a **block-structured transformation**, where:\n- Input dimension is partitioned into $h$ blocks\n- Each block is transformed independently\n- Results are concatenated and linearly combined\n\n**Connection to group theory:** This structure is related to **block-diagonal matrices** and **direct sum decompositions** in linear algebra - we're decomposing the transformation into independent subproblems.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Linear layers for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def split_heads(self, x):\n",
        "        \"\"\"Split into multiple heads.\"\"\"\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"Combine multiple heads.\"\"\"\n",
        "        batch_size, _, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear transformations\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "        \n",
        "        # Split into heads\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "        \n",
        "        # Attention for each head\n",
        "        output, attention = scaled_dot_product_attention(Q, K, V)\n",
        "        \n",
        "        # Combine heads\n",
        "        output = self.combine_heads(output)\n",
        "        \n",
        "        # Final linear\n",
        "        output = self.W_o(output)\n",
        "        \n",
        "        return output, attention\n",
        "\n",
        "# Test\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "output, attention = mha(x)\n",
        "\n",
        "print('Multi-Head Attention:')\n",
        "print(f'Number of heads: {num_heads}')\n",
        "print(f'Model dimension: {d_model}')\n",
        "print(f'Dimension per head: {d_model // num_heads}')\n",
        "print(f'\\nInput shape: {x.shape}')\n",
        "print(f'Output shape: {output.shape}')\n",
        "print(f'Attention shape: {attention.shape}')\n",
        "print('\\n\u2713 Each head attends differently!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Positional Encoding <a id=\"positional\"></a>\n\n### The Problem\n\nAttention has no notion of position! \"ACGT\" vs \"TGCA\" would be treated the same.\n\n### Solution: Positional Encoding\n\nAdd position information to embeddings:\n\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n\nwhere:\n- $pos$ = position in sequence\n- $i$ = dimension index\n- $d$ = model dimension\n\n### Why Sinusoidal?\n\n- Allows model to learn relative positions\n- Works for sequences longer than training\n- Smooth, continuous representation\n\n### Positional Encoding: Fourier Analysis Connection\n\n#### Connection to Fourier Series\n\nThe sinusoidal positional encoding:\n$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$$\n$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$$\n\nis directly inspired by **Fourier analysis** [@strang2016introduction]!\n\n**Classical Fourier series:** Any periodic function can be represented as:\n$$f(t) = a_0 + \\sum_{n=1}^{\\infty} [a_n \\cos(n\\omega t) + b_n \\sin(n\\omega t)]$$\n\n**Positional encoding:** Uses different frequencies for different dimensions:\n- Low dimensions: High frequency (capture fine-grained position differences)\n- High dimensions: Low frequency (capture coarse position information)\n\n**Why this works:**\n- **Completeness:** Fourier bases can represent any function (universal approximation)\n- **Orthogonality:** Different frequencies are orthogonal \u2192 independent information\n- **Smooth interpolation:** Sinusoidal functions generalize well to unseen positions\n- **Relative position:** $PE_{pos+k}$ can be written as a linear function of $PE_{pos}$ (enables learning relative positions)\n\n#### Connection to Feature Engineering in Time Series\n\nIn classical time series analysis [@hastie2009elements], we often add temporal features:\n- Hour of day (cyclical: 0-23)\n- Day of week (cyclical: 0-6)\n- Month of year (cyclical: 0-11)\n\nFor cyclical features, we use **sine-cosine encoding**:\n$$x_{\\text{hour}} \\rightarrow [\\sin(2\\pi \\cdot x_{\\text{hour}}/24), \\cos(2\\pi \\cdot x_{\\text{hour}}/24)]$$\n\n**Positional encoding does the same thing** but:\n- Uses multiple frequencies (not just one cycle)\n- Applies to abstract sequence positions (not necessarily time)\n- Provides a rich, high-dimensional positional representation\n\n#### Alternative View: Positional Basis Functions\n\nPositional encoding provides a set of **basis functions** over position space:\n- Each dimension is a basis function (sine or cosine of different frequency)\n- Position is represented as coordinates in this basis\n- Similar to representing functions in terms of basis functions in functional analysis\n\nThis is analogous to:\n- **Polynomial bases:** $1, x, x^2, x^3, ...$ (used in polynomial regression)\n- **Radial basis functions (RBF):** $\\exp(-||x - c||^2)$ (used in RBF networks)\n- **Wavelet bases:** Multiscale basis functions (used in signal processing)\n\nThe transformer learns to use this positional basis in combination with content information to solve the task.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding using sinusoidal functions.\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                           (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Add positional encoding\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Visualize positional encoding\n",
        "d_model = 128\n",
        "max_len = 50\n",
        "pos_enc = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# Get encodings\n",
        "encodings = pos_enc.pe[0, :max_len, :].numpy()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.imshow(encodings.T, cmap='RdBu', aspect='auto')\n",
        "plt.colorbar(label='Encoding Value')\n",
        "plt.xlabel('Position in Sequence', fontsize=12)\n",
        "plt.ylabel('Encoding Dimension', fontsize=12)\n",
        "plt.title('Positional Encoding Visualization', fontsize=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n\ud83d\udca1 Key Properties:')\n",
        "print('  - Each position has unique encoding')\n",
        "print('  - Different frequencies for different dimensions')\n",
        "print('  - Allows model to learn relative positions')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}