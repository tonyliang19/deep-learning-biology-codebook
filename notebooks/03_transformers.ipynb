{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Transformers\n",
    "\n",
    "Welcome to Chapter 3! Here we explore the revolutionary Transformer architecture that has transformed NLP and is now impacting biology.\n",
    "\n",
    "## \ud83d\udcda Table of Contents\n",
    "1. [Introduction to Attention](#intro)\n",
    "2. [Self-Attention Mechanism](#self-attention)\n",
    "3. [Multi-Head Attention](#multi-head)\n",
    "4. [Positional Encoding](#positional)\n",
    "5. [Complete Transformer Architecture](#architecture)\n",
    "6. [BERT and GPT](#bert-gpt)\n",
    "7. [Biology Application: Protein Sequence Analysis](#biology-app)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print('\u2713 Libraries imported')\n",
    "print(f'PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Attention <a id=\"intro\"></a>\n",
    "\n",
    "### The Problem with RNNs\n",
    "\n",
    "Traditional RNNs process sequences sequentially:\n",
    "- **Slow**: Cannot parallelize\n",
    "- **Memory**: Struggle with long sequences\n",
    "- **Gradient**: Vanishing/exploding gradients\n",
    "\n",
    "### The Attention Solution\n",
    "\n",
    "**Key Idea**: Let the model learn which parts of the input to focus on.\n",
    "\n",
    "**Attention Formula**:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "where:\n",
    "- $Q$ = Query (what we're looking for)\n",
    "- $K$ = Key (what we have)\n",
    "- $V$ = Value (what we return)\n",
    "- $d_k$ = dimension of keys (for scaling)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Think of it like a dictionary lookup:\n",
    "1. **Query**: \"What am I looking for?\"\n",
    "2. **Key**: \"Does this match what you want?\"\n",
    "3. **Value**: \"Here's the information\"\n",
    "\n",
    "The attention mechanism computes similarity between query and all keys, then returns a weighted sum of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (batch, seq_len, d_k)\n",
    "        K: Key matrix (batch, seq_len, d_k)\n",
    "        V: Value matrix (batch, seq_len, d_v)\n",
    "        mask: Optional mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention scores\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with simple example\n",
    "print('Testing Attention Mechanism:')\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "Q = torch.randn(1, seq_len, d_model)\n",
    "K = torch.randn(1, seq_len, d_model)\n",
    "V = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f'\\nInput shapes:')\n",
    "print(f'  Q: {Q.shape}')\n",
    "print(f'  K: {K.shape}')\n",
    "print(f'  V: {V.shape}')\n",
    "print(f'\\nOutput shapes:')\n",
    "print(f'  Output: {output.shape}')\n",
    "print(f'  Attention weights: {weights.shape}')\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(weights[0].detach().numpy(), annot=True, fmt='.2f', \n",
    "            cmap='YlOrRd', cbar_kws={'label': 'Attention Weight'})\n",
    "plt.xlabel('Key Position', fontsize=12)\n",
    "plt.ylabel('Query Position', fontsize=12)\n",
    "plt.title('Attention Weight Matrix', fontsize=14, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\ud83d\udca1 Each row shows where each query attends to!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Attention Mechanism <a id=\"self-attention\"></a>\n",
    "\n",
    "In **self-attention**, Q, K, and V all come from the same input!\n",
    "\n",
    "### Why Self-Attention?\n",
    "\n",
    "Allows each position to attend to all positions in the sequence:\n",
    "- Capture long-range dependencies\n",
    "- Parallel computation\n",
    "- No distance bias\n",
    "\n",
    "### How it Works\n",
    "\n",
    "1. Start with input embeddings $X$\n",
    "2. Create Q, K, V by linear transformations:\n",
    "   - $Q = XW_Q$\n",
    "   - $K = XW_K$\n",
    "   - $V = XW_V$\n",
    "3. Compute attention\n",
    "\n",
    "### Example: Protein Sequence\n",
    "\n",
    "For sequence \"ACGT\":\n",
    "- A might attend strongly to C (if they interact)\n",
    "- G might attend to T (complementary bases)\n",
    "- Self-attention learns these relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Linear transformations for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Create Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Compute attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test\n",
    "d_model = 64\n",
    "seq_len = 5\n",
    "batch_size = 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "self_attn = SelfAttention(d_model)\n",
    "\n",
    "output, weights = self_attn(x)\n",
    "\n",
    "print('Self-Attention Layer:')\n",
    "print(f'Input shape: {x.shape}')\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Attention weights shape: {weights.shape}')\n",
    "print('\\n\u2713 Self-attention preserves sequence length!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention <a id=\"multi-head\"></a>\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Single attention focuses on one aspect. Multiple heads allow:\n",
    "- Different representation subspaces\n",
    "- Attend to different positions\n",
    "- Capture various relationships\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "### Analogy\n",
    "\n",
    "Think of multiple editors reviewing the same text:\n",
    "- One checks grammar\n",
    "- One checks style  \n",
    "- One checks content\n",
    "- Combined feedback is comprehensive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split into multiple heads.\"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine multiple heads.\"\"\"\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear transformations\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Split into heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Attention for each head\n",
    "        output, attention = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Combine heads\n",
    "        output = self.combine_heads(output)\n",
    "        \n",
    "        # Final linear\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "# Test\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "output, attention = mha(x)\n",
    "\n",
    "print('Multi-Head Attention:')\n",
    "print(f'Number of heads: {num_heads}')\n",
    "print(f'Model dimension: {d_model}')\n",
    "print(f'Dimension per head: {d_model // num_heads}')\n",
    "print(f'\\nInput shape: {x.shape}')\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Attention shape: {attention.shape}')\n",
    "print('\\n\u2713 Each head attends differently!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding <a id=\"positional\"></a>\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Attention has no notion of position! \"ACGT\" vs \"TGCA\" would be treated the same.\n",
    "\n",
    "### Solution: Positional Encoding\n",
    "\n",
    "Add position information to embeddings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "where:\n",
    "- $pos$ = position in sequence\n",
    "- $i$ = dimension index\n",
    "- $d$ = model dimension\n",
    "\n",
    "### Why Sinusoidal?\n",
    "\n",
    "- Allows model to learn relative positions\n",
    "- Works for sequences longer than training\n",
    "- Smooth, continuous representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding using sinusoidal functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# Visualize positional encoding\n",
    "d_model = 128\n",
    "max_len = 50\n",
    "pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Get encodings\n",
    "encodings = pos_enc.pe[0, :max_len, :].numpy()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(encodings.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position in Sequence', fontsize=12)\n",
    "plt.ylabel('Encoding Dimension', fontsize=12)\n",
    "plt.title('Positional Encoding Visualization', fontsize=14, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\ud83d\udca1 Key Properties:')\n",
    "print('  - Each position has unique encoding')\n",
    "print('  - Different frequencies for different dimensions')\n",
    "print('  - Allows model to learn relative positions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}