{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: CNN for DNA Sequence Classification\n",
    "\n",
    "Welcome to your first complete biology project! \ud83e\uddec\n",
    "\n",
    "In this notebook, we'll apply CNNs (from Chapter 2) to classify DNA sequences. Specifically, we'll build a model that can identify **promoter regions** - special DNA sequences that control when genes are turned on or off.\n",
    "\n",
    "## \ud83c\udfaf The Biological Problem\n",
    "\n",
    "### What are Promoters?\n",
    "\n",
    "Think of DNA as an instruction manual for building proteins. But how does a cell know WHEN to read each instruction? That's where promoters come in:\n",
    "\n",
    "- **Promoters** are DNA sequences (typically 100-1000 base pairs) located upstream of genes\n",
    "- They act like \"ON switches\" for genes\n",
    "- RNA polymerase (the enzyme that reads DNA) binds to promoters to start transcription\n",
    "- Promoters often contain recognizable patterns called **motifs** (like \"TATA box\": TATAAA)\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Identifying promoters helps us:\n",
    "- Understand gene regulation (what controls gene activity)\n",
    "- Predict gene expression patterns\n",
    "- Design synthetic biology systems\n",
    "- Diagnose diseases caused by promoter mutations\n",
    "\n",
    "### The Machine Learning Task\n",
    "\n",
    "**Input:** A DNA sequence (string of A, T, G, C)\n",
    "```\n",
    "ATGCGATATATAAAGCTAGC...\n",
    "```\n",
    "\n",
    "**Output:** Is this a promoter region? (Yes/No)\n",
    "\n",
    "**Challenge:** Promoters don't have exact sequences - they have patterns that can vary. Perfect job for deep learning!\n",
    "\n",
    "## \ud83d\udcda What You'll Learn\n",
    "\n",
    "1. **Data Representation:** How to convert DNA sequences (ATGC) into numbers that neural networks can process\n",
    "2. **One-Hot Encoding:** The standard way to represent categorical data\n",
    "3. **1D Convolutions:** Like 2D convolutions for images, but for sequences\n",
    "4. **Motif Detection:** How CNNs automatically learn to recognize important DNA patterns\n",
    "5. **Evaluation:** How to measure if your model is working\n",
    "\n",
    "## \ud83d\udd27 Skills You'll Practice\n",
    "\n",
    "- Preparing biological sequence data\n",
    "- Building and training a CNN for sequences\n",
    "- Visualizing what the network learned\n",
    "- Interpreting results in biological context\n",
    "\n",
    "Let's get started! \ud83d\ude80\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries. We'll use PyTorch as our deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation\n",
    "\n",
    "Since we're using synthetic data for educational purposes, we'll create a simple dataset:\n",
    "\n",
    "- **Promoter sequences**: Will contain common promoter motifs like TATA box (\"TATAAA\")\n",
    "- **Non-promoter sequences**: Random DNA sequences\n",
    "\n",
    "In real applications, you would load data from databases like DBTSS or EPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_promoter_sequence(length=200):\n",
    "    \"\"\"\n",
    "    Generate a synthetic promoter sequence with common motifs.\n",
    "    Real promoters often contain TATA box (~25-30bp upstream of TSS)\n",
    "    \"\"\"\n",
    "    bases = ['A', 'T', 'C', 'G']\n",
    "    sequence = [np.random.choice(bases) for _ in range(length)]\n",
    "    \n",
    "    # Insert TATA box motif at a random position\n",
    "    tata_box = list(\"TATAAA\")\n",
    "    insert_pos = np.random.randint(20, length - 30)\n",
    "    sequence[insert_pos:insert_pos + len(tata_box)] = tata_box\n",
    "    \n",
    "    # Sometimes add CAAT box\n",
    "    if np.random.random() > 0.5:\n",
    "        caat_box = list(\"CCAAT\")\n",
    "        insert_pos = np.random.randint(40, length - 20)\n",
    "        sequence[insert_pos:insert_pos + len(caat_box)] = caat_box\n",
    "    \n",
    "    return ''.join(sequence)\n",
    "\n",
    "def generate_non_promoter_sequence(length=200):\n",
    "    \"\"\"\n",
    "    Generate a random DNA sequence without promoter motifs.\n",
    "    \"\"\"\n",
    "    bases = ['A', 'T', 'C', 'G']\n",
    "    return ''.join([np.random.choice(bases) for _ in range(length)])\n",
    "\n",
    "# Generate dataset\n",
    "n_samples = 1000\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    if i < n_samples // 2:\n",
    "        sequences.append(generate_promoter_sequence())\n",
    "        labels.append(1)  # Promoter\n",
    "    else:\n",
    "        sequences.append(generate_non_promoter_sequence())\n",
    "        labels.append(0)  # Non-promoter\n",
    "\n",
    "print(f\"Generated {len(sequences)} sequences\")\n",
    "print(f\"Example promoter sequence: {sequences[0][:50]}...\")\n",
    "print(f\"Example non-promoter sequence: {sequences[-1][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DNA Sequence Encoding\n",
    "\n",
    "Neural networks work with numbers, not letters. We need to convert DNA sequences into numerical representations.\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "We'll use **one-hot encoding**, where each nucleotide is represented as a 4-dimensional vector:\n",
    "- A = [1, 0, 0, 0]\n",
    "- T = [0, 1, 0, 0]\n",
    "- C = [0, 0, 1, 0]\n",
    "- G = [0, 0, 0, 1]\n",
    "\n",
    "A sequence of length L becomes a matrix of shape (4, L), which is perfect for CNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence):\n",
    "    \"\"\"\n",
    "    Convert DNA sequence to one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        sequence: DNA sequence string\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (4, len(sequence))\n",
    "    \"\"\"\n",
    "    mapping = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "    seq_len = len(sequence)\n",
    "    one_hot = np.zeros((4, seq_len), dtype=np.float32)\n",
    "    \n",
    "    for i, nucleotide in enumerate(sequence):\n",
    "        if nucleotide in mapping:\n",
    "            one_hot[mapping[nucleotide], i] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "# Test the encoding\n",
    "test_seq = \"ATCG\"\n",
    "encoded = one_hot_encode(test_seq)\n",
    "print(f\"Sequence: {test_seq}\")\n",
    "print(f\"Encoded shape: {encoded.shape}\")\n",
    "print(f\"Encoded matrix:\\n{encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch Dataset Class\n",
    "\n",
    "We create a custom Dataset class to handle our sequences efficiently. This is PyTorch's standard way of organizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNASequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for DNA sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Encode sequence\n",
    "        encoded = one_hot_encode(sequence)\n",
    "        \n",
    "        return torch.tensor(encoded, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create dataset\n",
    "dataset = DNASequenceDataset(sequences, labels)\n",
    "\n",
    "# Split into train, validation, and test sets (70%, 15%, 15%)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders\n",
    "\n",
    "DataLoaders handle batching and shuffling of our data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the CNN Model\n",
    "\n",
    "Now for the exciting part - building our CNN!\n",
    "\n",
    "### Architecture Components:\n",
    "\n",
    "1. **Conv1d Layers**: Detect local patterns (motifs) in sequences\n",
    "   - First layer: Detects simple motifs (3-6 nucleotides)\n",
    "   - Second layer: Detects combinations of motifs\n",
    "\n",
    "2. **MaxPooling**: Reduces dimensionality and provides position invariance\n",
    "\n",
    "3. **Fully Connected Layers**: Combine detected features for final classification\n",
    "\n",
    "4. **Dropout**: Prevents overfitting by randomly dropping neurons during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNASequenceCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for DNA sequence classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length=200, num_classes=2):\n",
    "        super(DNASequenceCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        # Input: (batch, 4, 200) - 4 channels (A,T,C,G), sequence length 200\n",
    "        # Kernel size 6: looks at 6 nucleotides at a time (typical motif length)\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=6, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        # Detects more complex patterns by combining first-layer features\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=6, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=6, padding=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate the size after convolutions and pooling\n",
    "        # Original: 200 -> after pool1: 100 -> after pool2: 50 -> after pool3: 25\n",
    "        self.fc_input_size = 128 * 24  # 24 because of padding adjustments\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Randomly drop 50% of neurons during training\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.dropout(self.relu4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and move to GPU if available\n",
    "model = DNASequenceCNN().to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "\n",
    "### Loss Function: Cross-Entropy Loss\n",
    "Suitable for classification tasks. It measures how well our predictions match the true labels.\n",
    "\n",
    "### Optimizer: Adam\n",
    "An adaptive learning rate optimizer that works well for most problems. It's like a smart way to update model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler: reduces learning rate when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "The training loop:\n",
    "1. **Forward pass**: Feed data through the network\n",
    "2. **Calculate loss**: How wrong are our predictions?\n",
    "3. **Backward pass**: Calculate gradients (how to adjust weights)\n",
    "4. **Update weights**: Make the model better\n",
    "\n",
    "We'll track both training and validation metrics to ensure the model generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in tqdm(loader, desc=\"Training\"):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress\n",
    "\n",
    "Let's plot the training and validation metrics to understand how well our model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot losses\n",
    "ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax1.plot(val_losses, label='Validation Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
    "ax2.plot(val_accs, label='Validation Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"- Training loss should decrease over time\")\n",
    "print(\"- If validation loss increases while training loss decreases, we have overfitting\")\n",
    "print(\"- Similar train and val accuracy indicates good generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set\n",
    "\n",
    "Finally, we evaluate on the test set that the model has never seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_probabilities)\n",
    "\n",
    "# Evaluate\n",
    "test_predictions, test_labels, test_probabilities = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=['Non-Promoter', 'Promoter']))\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = 100 * np.sum(test_predictions == test_labels) / len(test_labels)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrix\n",
    "\n",
    "A confusion matrix shows where our model makes mistakes:\n",
    "- **True Positives (TP)**: Correctly identified promoters\n",
    "- **True Negatives (TN)**: Correctly identified non-promoters\n",
    "- **False Positives (FP)**: Non-promoters incorrectly labeled as promoters\n",
    "- **False Negatives (FN)**: Promoters incorrectly labeled as non-promoters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Promoter', 'Promoter'],\n",
    "            yticklabels=['Non-Promoter', 'Promoter'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ROC Curve\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve shows the trade-off between sensitivity and specificity. A perfect classifier has AUC = 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_probabilities[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Learned Filters\n",
    "\n",
    "Let's examine what patterns (motifs) the first convolutional layer has learned to detect. These should resemble biological motifs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first layer filters\n",
    "first_layer_weights = model.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "# Plot first 8 filters\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "nucleotides = ['A', 'T', 'C', 'G']\n",
    "\n",
    "for i in range(8):\n",
    "    filter_weights = first_layer_weights[i]  # Shape: (4, kernel_size)\n",
    "    \n",
    "    # Create visualization\n",
    "    im = axes[i].imshow(filter_weights, cmap='RdBu_r', aspect='auto')\n",
    "    axes[i].set_yticks(range(4))\n",
    "    axes[i].set_yticklabels(nucleotides)\n",
    "    axes[i].set_xlabel('Position in motif')\n",
    "    axes[i].set_title(f'Filter {i+1}')\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Learned Convolutional Filters (Motif Detectors)', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Red: Strong positive weights (filter activates for these nucleotides)\")\n",
    "print(\"- Blue: Strong negative weights (filter suppresses for these nucleotides)\")\n",
    "print(\"- These patterns should resemble known promoter motifs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. \u2705 **Encoded DNA sequences** using one-hot encoding for neural network input\n",
    "2. \u2705 **Built a CNN architecture** specifically designed for sequence classification\n",
    "3. \u2705 **Trained the model** with proper train/validation/test splits\n",
    "4. \u2705 **Evaluated performance** using multiple metrics (accuracy, confusion matrix, ROC curve)\n",
    "5. \u2705 **Visualized learned patterns** to understand what the model detects\n",
    "\n",
    "### Why This Approach Works:\n",
    "\n",
    "- **CNNs excel at pattern recognition**: Perfect for finding motifs in sequences\n",
    "- **Translation invariance**: Detects motifs regardless of their position\n",
    "- **Parameter efficiency**: Shared weights across the sequence reduce overfitting\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Use real promoter databases (e.g., EPD, DBTSS)\n",
    "- Experiment with different architectures\n",
    "- Try attention mechanisms to identify important regions\n",
    "- Apply to other sequence classification tasks (splice sites, transcription factor binding)\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- Gene annotation\n",
    "- Regulatory element prediction\n",
    "- Variant effect prediction\n",
    "- Drug target identification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}