{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Vision Transformers (ViT)\n",
    "\n",
    "Welcome to the cutting edge of computer vision! \ud83d\udd2c\n",
    "\n",
    "In 2020, researchers asked: \"Can we apply transformers (from Chapter 3) to images?\" The answer was a resounding YES! Vision Transformers (ViT) now compete with or beat CNNs on many tasks.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to turn images into sequences (so transformers can process them)\n",
    "- The Vision Transformer architecture\n",
    "- When to use ViT vs CNN\n",
    "- Apply ViT to medical/biological images\n",
    "\n",
    "**Prerequisites:**\n",
    "- Chapter 2 (CNNs) - to understand the comparison\n",
    "- Chapter 3 (Transformers) - ViT builds directly on these concepts\n",
    "\n",
    "**The Big Idea:** Instead of treating an image as a grid of pixels, we break it into patches (like puzzle pieces) and feed them to a transformer!\n",
    "\n",
    "## \ud83d\udcda Table of Contents\n",
    "1. [From CNNs to Transformers](#cnn-to-vit)\n",
    "2. [Patch Embedding - Breaking Images into Pieces](#patch-embedding)\n",
    "3. [Vision Transformer Architecture](#architecture)\n",
    "4. [Position Embeddings for Images](#position)\n",
    "5. [ViT vs CNN: When to Use Each](#comparison)\n",
    "6. [Hybrid Architectures](#hybrid)\n",
    "7. [Biology Application: Medical Image Analysis](#biology-app)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print('\u2713 Libraries imported')\n",
    "print(f'PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From CNNs to Transformers <a id=\"cnn-to-vit\"></a>\n",
    "\n",
    "### CNNs: The Traditional Approach\n",
    "\n",
    "**Strengths**:\n",
    "- Strong inductive biases (locality, translation invariance)\n",
    "- Efficient for images\n",
    "- Well-established architectures\n",
    "\n",
    "**Limitations**:\n",
    "- Limited receptive field\n",
    "- Fixed spatial relationships\n",
    "- Difficulty with long-range dependencies\n",
    "\n",
    "### Vision Transformers: The New Paradigm\n",
    "\n",
    "**Key Idea**: Treat an image as a sequence of patches!\n",
    "\n",
    "**Process**:\n",
    "1. Split image into patches (e.g., 16\u00d716)\n",
    "2. Flatten each patch into a vector\n",
    "3. Add positional embeddings\n",
    "4. Feed through Transformer encoder\n",
    "\n",
    "**Benefits**:\n",
    "- Global receptive field from layer 1\n",
    "- Flexible attention patterns\n",
    "- Scalable with data\n",
    "\n",
    "### The Trade-off\n",
    "\n",
    "- **CNNs**: Better with small datasets (strong inductive bias)\n",
    "- **ViT**: Better with large datasets (learns from data)\n",
    "\n",
    "### Paper: \"An Image is Worth 16x16 Words\"\n",
    "\n",
    "This groundbreaking 2020 paper showed ViT can match or exceed CNNs when trained on sufficient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patch_extraction():\n",
    "    \"\"\"Visualize how an image is split into patches.\"\"\"\n",
    "    \n",
    "    # Create a sample image (8x8 for visualization)\n",
    "    image = np.random.rand(8, 8, 3)\n",
    "    patch_size = 2\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Original image with grid\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image (8\u00d78)', fontsize=14, weight='bold')\n",
    "    \n",
    "    # Draw patch boundaries\n",
    "    for i in range(0, 8, patch_size):\n",
    "        axes[0].axhline(y=i-0.5, color='red', linewidth=2)\n",
    "        axes[0].axvline(x=i-0.5, color='red', linewidth=2)\n",
    "    axes[0].axhline(y=7.5, color='red', linewidth=2)\n",
    "    axes[0].axvline(x=7.5, color='red', linewidth=2)\n",
    "    \n",
    "    # Number patches\n",
    "    patch_num = 0\n",
    "    for i in range(0, 8, patch_size):\n",
    "        for j in range(0, 8, patch_size):\n",
    "            axes[0].text(j + patch_size/2 - 0.5, i + patch_size/2 - 0.5, \n",
    "                        str(patch_num), ha='center', va='center',\n",
    "                        fontsize=16, weight='bold', color='yellow',\n",
    "                        bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "            patch_num += 1\n",
    "    \n",
    "    axes[0].set_xticks([])\n",
    "    axes[0].set_yticks([])\n",
    "    \n",
    "    # Extract and visualize patches as sequence\n",
    "    num_patches_per_dim = 8 // patch_size\n",
    "    num_patches = num_patches_per_dim ** 2\n",
    "    \n",
    "    # Show patches as a sequence\n",
    "    patch_display = np.zeros((patch_size * 2, patch_size * num_patches, 3))\n",
    "    patch_num = 0\n",
    "    for i in range(0, 8, patch_size):\n",
    "        for j in range(0, 8, patch_size):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size, :]\n",
    "            col_start = patch_num * patch_size\n",
    "            patch_display[0:patch_size, col_start:col_start+patch_size, :] = patch\n",
    "            patch_display[patch_size:, col_start:col_start+patch_size, :] = patch\n",
    "            patch_num += 1\n",
    "    \n",
    "    axes[1].imshow(patch_display)\n",
    "    axes[1].set_title(f'Patch Sequence ({num_patches} patches)', fontsize=14, weight='bold')\n",
    "    axes[1].set_xlabel('Patch sequence \u2192', fontsize=12)\n",
    "    axes[1].set_xticks([])\n",
    "    axes[1].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\n\ud83d\udcca Patch Statistics:')\n",
    "    print(f'  Image size: 8\u00d78\u00d73 = {8*8*3} values')\n",
    "    print(f'  Patch size: {patch_size}\u00d7{patch_size}')\n",
    "    print(f'  Number of patches: {num_patches}')\n",
    "    print(f'  Each patch: {patch_size}\u00d7{patch_size}\u00d73 = {patch_size*patch_size*3} values')\n",
    "    print(f'\\n  For standard ViT (224\u00d7224 image, 16\u00d716 patches):')\n",
    "    print(f'    Number of patches: (224/16)\u00b2 = 196 patches')\n",
    "    print(f'    Sequence length: 196 + 1 (class token) = 197')\n",
    "\n",
    "visualize_patch_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Patch Embeddings <a id=\"patch-embed\"></a>\n",
    "\n",
    "### Converting Images to Sequences\n",
    "\n",
    "Given an image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$:\n",
    "\n",
    "1. **Split into patches**: $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times (P^2 \\cdot C)}$\n",
    "   - $N = HW/P^2$ (number of patches)\n",
    "   - $P$ = patch size\n",
    "\n",
    "2. **Linear projection**: $\\mathbf{z}_0 = [\\mathbf{x}_{class}; \\mathbf{x}_p^1\\mathbf{E}; ...; \\mathbf{x}_p^N\\mathbf{E}] + \\mathbf{E}_{pos}$\n",
    "   - $\\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$ (patch embedding)\n",
    "   - $\\mathbf{E}_{pos} \\in \\mathbb{R}^{(N+1) \\times D}$ (positional embedding)\n",
    "   - $\\mathbf{x}_{class}$ = learnable class token\n",
    "\n",
    "### Class Token\n",
    "\n",
    "A learnable embedding prepended to the sequence:\n",
    "- Aggregates information from all patches\n",
    "- Used for final classification\n",
    "- Similar to [CLS] token in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image to patch embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Use Conv2d for efficiency (equivalent to splitting and linear projection)\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, \n",
    "            embed_dim, \n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels, height, width)\n",
    "        x = self.proj(x)  # (batch_size, embed_dim, n_patches**0.5, n_patches**0.5)\n",
    "        x = x.flatten(2)  # (batch_size, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "# Test patch embedding\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "in_channels = 3\n",
    "embed_dim = 768\n",
    "batch_size = 4\n",
    "\n",
    "patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "\n",
    "# Create sample images\n",
    "images = torch.randn(batch_size, in_channels, img_size, img_size)\n",
    "\n",
    "# Get patch embeddings\n",
    "embeddings = patch_embed(images)\n",
    "\n",
    "print('Patch Embedding:')\n",
    "print(f'Input image shape: {images.shape}')\n",
    "print(f'  (batch_size, channels, height, width)')\n",
    "print(f'\\nOutput embedding shape: {embeddings.shape}')\n",
    "print(f'  (batch_size, n_patches, embed_dim)')\n",
    "print(f'\\nNumber of patches: {patch_embed.n_patches}')\n",
    "print(f'Embedding dimension: {embed_dim}')\n",
    "print('\\n\u2713 Image successfully converted to sequence!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vision Transformer Architecture <a id=\"vit-arch\"></a>\n",
    "\n",
    "### Complete ViT Pipeline\n",
    "\n",
    "```\n",
    "Image (224\u00d7224\u00d73)\n",
    "    \u2193\n",
    "Patch Embedding (196 patches of 16\u00d716)\n",
    "    \u2193\n",
    "Add Class Token (197 tokens)\n",
    "    \u2193\n",
    "Add Positional Embedding\n",
    "    \u2193\n",
    "Transformer Encoder (L layers)\n",
    "    \u2193\n",
    "Extract Class Token\n",
    "    \u2193\n",
    "MLP Head (Classification)\n",
    "    \u2193\n",
    "Output (num_classes)\n",
    "```\n",
    "\n",
    "### Transformer Encoder Block\n",
    "\n",
    "Each block contains:\n",
    "1. Layer Normalization\n",
    "2. Multi-Head Self-Attention\n",
    "3. Residual connection\n",
    "4. Layer Normalization\n",
    "5. MLP (Feed-Forward)\n",
    "6. Residual connection\n",
    "\n",
    "### ViT Variants\n",
    "\n",
    "- **ViT-Base**: 12 layers, 768 dim, 12 heads, 86M params\n",
    "- **ViT-Large**: 24 layers, 1024 dim, 16 heads, 307M params\n",
    "- **ViT-Huge**: 32 layers, 1280 dim, 16 heads, 632M params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified Vision Transformer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
    "                 num_classes=1000, embed_dim=768, depth=12, num_heads=12, \n",
    "                 mlp_ratio=4.0, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, N, D)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, N+1, D)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Classification from class token\n",
    "        x = self.norm(x[:, 0])  # Take class token\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create ViT-Tiny for demonstration\n",
    "model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=10,  # Reduced for demo\n",
    "    embed_dim=192,   # Smaller model\n",
    "    depth=6,         # Fewer layers\n",
    "    num_heads=3,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "output = model(x)\n",
    "\n",
    "print('Vision Transformer:')\n",
    "print(f'Input shape: {x.shape}')\n",
    "print(f'Output shape: {output.shape}')\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'\\nTotal parameters: {total_params:,}')\n",
    "print('\\n\u2713 Complete ViT model!')\n",
    "print('\\n\ud83d\udca1 For comparison:')\n",
    "print('  ViT-Base: ~86M parameters')\n",
    "print('  ResNet-50: ~25M parameters')\n",
    "print('  ViT needs more data but scales better!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ViT vs CNN: When to Use Each <a id=\"comparison\"></a>\n",
    "\n",
    "### When to Use CNNs\n",
    "\n",
    "\u2705 **Small to medium datasets** (< 1M images)\n",
    "\u2705 **Limited computational resources**\n",
    "\u2705 **Need for translation invariance** (strong inductive bias)\n",
    "\u2705 **Real-time inference** (faster)\n",
    "\u2705 **Local pattern recognition**\n",
    "\n",
    "### When to Use ViT\n",
    "\n",
    "\u2705 **Large datasets** (> 10M images)\n",
    "\u2705 **Abundant computational resources**\n",
    "\u2705 **Global context important** (long-range dependencies)\n",
    "\u2705 **Transfer learning from large models**\n",
    "\u2705 **When you can pretrain on huge datasets**\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Aspect | CNN (ResNet) | ViT |\n",
    "|--------|--------------|-----|\n",
    "| Inductive bias | Strong (locality) | Weak |\n",
    "| Data requirement | Low | High |\n",
    "| Training time | Faster | Slower |\n",
    "| Inference speed | Faster | Slower |\n",
    "| Scalability | Limited | Excellent |\n",
    "| Interpretability | Medium | High (attention maps) |\n",
    "\n",
    "### Biology Applications\n",
    "\n",
    "**Use CNNs for**:\n",
    "- Cell counting (limited data)\n",
    "- Quick microscopy analysis\n",
    "- Real-time diagnosis\n",
    "\n",
    "**Use ViT for**:\n",
    "- Large medical image datasets\n",
    "- Multi-scale tissue analysis\n",
    "- When pretrained models available\n",
    "- Complex pathology images\n",
    "\n",
    "### Practical Tip\n",
    "\n",
    "\ud83c\udfaf **Best of both worlds**: Use pretrained ViT models! They're trained on ImageNet or larger datasets (like JFT-300M) and work well even on small target datasets through transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways\n",
    "\n",
    "### Vision Transformers\n",
    "\n",
    "1. **Patch Embedding**: Treat images as sequences of patches\n",
    "2. **Self-Attention**: Global receptive field from layer 1\n",
    "3. **Class Token**: Learnable token for classification\n",
    "4. **Positional Encoding**: Preserve spatial information\n",
    "5. **Scalability**: Performance improves with data and model size\n",
    "\n",
    "### Design Choices\n",
    "\n",
    "- **Patch Size**: 16\u00d716 is standard, smaller = more patches = more computation\n",
    "- **Model Size**: Bigger is better (with sufficient data)\n",
    "- **Pretraining**: Essential for good performance\n",
    "- **Regularization**: Important to prevent overfitting\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "1. **Efficiency**: Swin Transformer, DeiT (data-efficient)\n",
    "2. **Hybrid Models**: Combining CNNs and Transformers\n",
    "3. **Self-Supervised Learning**: MAE (Masked Autoencoders)\n",
    "4. **Multi-Modal**: CLIP (vision + language)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Congratulations!\n",
    "\n",
    "You've completed the Vision Transformers chapter! You now understand:\n",
    "- How to convert images to sequences\n",
    "- The complete ViT architecture\n",
    "- When to use ViT vs CNNs\n",
    "- How to implement ViT components\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Practice**: Implement ViT from scratch\n",
    "2. **Experiment**: Try different patch sizes\n",
    "3. **Apply**: Use pretrained ViT on your data\n",
    "4. **Explore**: Check out Swin Transformer, DeiT, DINO\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- Original ViT paper: \"An Image is Worth 16x16 Words\"\n",
    "- timm library: Pre-implemented ViT variants\n",
    "- Hugging Face Transformers: Easy-to-use ViT models\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83c\udf89 You've completed the Deep Learning Biology Codebook!**\n",
    "\n",
    "**You've learned**:\n",
    "- \u2705 Neural Networks fundamentals\n",
    "- \u2705 Convolutional Neural Networks\n",
    "- \u2705 Transformers architecture\n",
    "- \u2705 Vision Transformers\n",
    "\n",
    "**Keep learning and applying these powerful techniques to biological problems!** \ud83e\uddec\ud83e\udd16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}