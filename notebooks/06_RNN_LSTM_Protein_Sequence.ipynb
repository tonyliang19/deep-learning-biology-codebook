{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: RNN/LSTM for Protein Sequence Analysis\n",
    "\n",
    "Welcome to protein sequence analysis! \ud83e\uddea\n",
    "\n",
    "In this notebook, we'll use Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks to classify protein families. Proteins are the workhorses of biology, and understanding them is crucial for drug design, disease research, and biotechnology.\n",
    "\n",
    "## \ud83c\udfaf The Biological Problem\n",
    "\n",
    "### What are Proteins?\n",
    "\n",
    "Proteins are chains of amino acids (20 different types, represented by letters like A, G, W, etc.):\n",
    "\n",
    "```\n",
    "MKTAYIAKQRQISFVKSHFSRQLEERLGLIEV...\n",
    "```\n",
    "\n",
    "- **Length:** Typically 50-1000 amino acids (some much longer!)\n",
    "- **Function:** Determined by the sequence and 3D structure\n",
    "- **Families:** Proteins with similar sequences usually have similar functions\n",
    "\n",
    "### Why Classify Protein Families?\n",
    "\n",
    "When scientists discover a new protein sequence, they want to know:\n",
    "- What does it do? (enzyme, transport, signaling, etc.)\n",
    "- What family does it belong to?\n",
    "- Can we predict its structure or function?\n",
    "\n",
    "**Example families:**\n",
    "- Kinases (proteins that add phosphate groups)\n",
    "- G-protein coupled receptors (cell signaling)\n",
    "- Immunoglobulins (antibodies)\n",
    "\n",
    "### The Machine Learning Task\n",
    "\n",
    "**Input:** Protein sequence\n",
    "```\n",
    "MGAAASIQTTVNTLSERISSKLEQEANASAQTKCDIEIGNFYIRQNHGCNLTVKNMCSAD\n",
    "```\n",
    "\n",
    "**Output:** Which family does this belong to? (multi-class classification)\n",
    "\n",
    "**Challenge:** Sequences vary in length, and similar function can come from very different sequences!\n",
    "\n",
    "## \ud83d\udcda What You'll Learn\n",
    "\n",
    "1. **RNNs:** Networks that can process sequences of varying lengths\n",
    "2. **LSTMs:** Advanced RNNs that can remember long-range dependencies\n",
    "3. **Embeddings:** How to represent amino acids as vectors\n",
    "4. **Bidirectional Processing:** Reading sequences forward AND backward\n",
    "5. **Handling Variable Lengths:** Padding and masking techniques\n",
    "\n",
    "## \ud83d\udd27 Why RNNs/LSTMs for Sequences?\n",
    "\n",
    "Unlike CNNs (which look at local patterns), RNNs:\n",
    "- Process one amino acid at a time\n",
    "- Maintain a \"memory\" of what they've seen\n",
    "- Can handle sequences of any length\n",
    "- Capture long-range dependencies (amino acids far apart but functionally related)\n",
    "\n",
    "Let's dive in! \ud83d\ude80\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Amino Acids and Protein Encoding\n",
    "\n",
    "Proteins are made of 20 standard amino acids. Each has different properties:\n",
    "- **Hydrophobic**: A, V, I, L, M, F, W, P\n",
    "- **Polar**: S, T, N, Q, Y, C\n",
    "- **Charged**: K, R, H (positive), D, E (negative)\n",
    "- **Special**: G (flexible), P (rigid)\n",
    "\n",
    "We'll use integer encoding where each amino acid gets a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amino acid vocabulary\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: idx + 1 for idx, aa in enumerate(AMINO_ACIDS)}  # 0 reserved for padding\n",
    "aa_to_idx['<PAD>'] = 0\n",
    "idx_to_aa = {idx: aa for aa, idx in aa_to_idx.items()}\n",
    "\n",
    "print(\"Amino acid encoding:\")\n",
    "for aa, idx in list(aa_to_idx.items())[:5]:\n",
    "    print(f\"  {aa}: {idx}\")\n",
    "print(f\"  ...\")\n",
    "print(f\"Total vocabulary size: {len(aa_to_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Protein Sequences\n",
    "\n",
    "We'll create three simplified protein families:\n",
    "1. **Kinases**: Often have catalytic domains with specific motifs\n",
    "2. **Proteases**: Have active site residues (catalytic triad)\n",
    "3. **Transporters**: Hydrophobic regions for membrane spanning\n",
    "\n",
    "In practice, you would use databases like Pfam, UniProt, or SCOP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kinase_sequence(length=150):\n",
    "    \"\"\"\n",
    "    Generate synthetic kinase-like sequence.\n",
    "    Kinases often have ATP-binding motif (GXGXXG) and catalytic loop (DFG).\n",
    "    \"\"\"\n",
    "    sequence = [np.random.choice(list(AMINO_ACIDS)) for _ in range(length)]\n",
    "    \n",
    "    # Insert ATP-binding motif\n",
    "    motif = ['G', 'K', 'G', 'S', 'F', 'G']\n",
    "    pos = np.random.randint(10, 40)\n",
    "    sequence[pos:pos+len(motif)] = motif\n",
    "    \n",
    "    # Insert catalytic motif (DFG)\n",
    "    catalytic = ['D', 'F', 'G']\n",
    "    pos = np.random.randint(60, 90)\n",
    "    sequence[pos:pos+len(catalytic)] = catalytic\n",
    "    \n",
    "    return ''.join(sequence)\n",
    "\n",
    "def generate_protease_sequence(length=150):\n",
    "    \"\"\"\n",
    "    Generate synthetic protease-like sequence.\n",
    "    Proteases have catalytic triad (Ser-His-Asp or Cys-His-Asn).\n",
    "    \"\"\"\n",
    "    sequence = [np.random.choice(list(AMINO_ACIDS)) for _ in range(length)]\n",
    "    \n",
    "    # Insert catalytic triad residues at different positions\n",
    "    positions = [np.random.randint(20, 50), \n",
    "                 np.random.randint(60, 90), \n",
    "                 np.random.randint(100, 130)]\n",
    "    sequence[positions[0]] = 'S'  # Serine\n",
    "    sequence[positions[1]] = 'H'  # Histidine\n",
    "    sequence[positions[2]] = 'D'  # Aspartate\n",
    "    \n",
    "    return ''.join(sequence)\n",
    "\n",
    "def generate_transporter_sequence(length=150):\n",
    "    \"\"\"\n",
    "    Generate synthetic transporter-like sequence.\n",
    "    Transporters have hydrophobic transmembrane regions.\n",
    "    \"\"\"\n",
    "    hydrophobic_aa = 'AVILMFW'\n",
    "    sequence = []\n",
    "    \n",
    "    # Create alternating hydrophobic and mixed regions\n",
    "    pos = 0\n",
    "    while pos < length:\n",
    "        if pos % 40 < 20:  # Transmembrane region\n",
    "            sequence.append(np.random.choice(list(hydrophobic_aa)))\n",
    "        else:  # Cytoplasmic/extracellular region\n",
    "            sequence.append(np.random.choice(list(AMINO_ACIDS)))\n",
    "        pos += 1\n",
    "    \n",
    "    return ''.join(sequence)\n",
    "\n",
    "# Generate dataset\n",
    "n_samples_per_class = 400\n",
    "sequences = []\n",
    "labels = []\n",
    "label_names = ['Kinase', 'Protease', 'Transporter']\n",
    "\n",
    "for i in range(n_samples_per_class):\n",
    "    sequences.append(generate_kinase_sequence(length=np.random.randint(120, 180)))\n",
    "    labels.append(0)\n",
    "    \n",
    "    sequences.append(generate_protease_sequence(length=np.random.randint(120, 180)))\n",
    "    labels.append(1)\n",
    "    \n",
    "    sequences.append(generate_transporter_sequence(length=np.random.randint(120, 180)))\n",
    "    labels.append(2)\n",
    "\n",
    "print(f\"Generated {len(sequences)} protein sequences\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    count = labels.count(i)\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "print(f\"\\nExample sequences:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {label_names[i]}: {sequences[i * n_samples_per_class][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sequence Encoding and Padding\n",
    "\n",
    "RNNs can handle variable-length sequences, but batching requires padding:\n",
    "- All sequences in a batch must have the same length\n",
    "- We pad shorter sequences with zeros\n",
    "- We'll use PyTorch's `pack_padded_sequence` to handle this efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, aa_to_idx):\n",
    "    \"\"\"\n",
    "    Convert amino acid sequence to integers.\n",
    "    \"\"\"\n",
    "    return [aa_to_idx.get(aa, 0) for aa in sequence]\n",
    "\n",
    "def pad_sequences(sequences, max_length=None):\n",
    "    \"\"\"\n",
    "    Pad sequences to the same length.\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    padded = []\n",
    "    lengths = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        length = len(seq)\n",
    "        lengths.append(length)\n",
    "        padded.append(seq + [0] * (max_length - length))\n",
    "    \n",
    "    return padded, lengths\n",
    "\n",
    "# Test encoding\n",
    "test_seq = \"ACDEFG\"\n",
    "encoded = encode_sequence(test_seq, aa_to_idx)\n",
    "print(f\"Sequence: {test_seq}\")\n",
    "print(f\"Encoded: {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset for Variable-Length Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for protein sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences, labels, aa_to_idx):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.aa_to_idx = aa_to_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Encode sequence\n",
    "        encoded = encode_sequence(sequence, self.aa_to_idx)\n",
    "        length = len(encoded)\n",
    "        \n",
    "        return torch.tensor(encoded, dtype=torch.long), length, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences.\n",
    "    \"\"\"\n",
    "    # Sort batch by sequence length (required for pack_padded_sequence)\n",
    "    batch.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    sequences, lengths, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    sequences_padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return sequences_padded, torch.tensor(lengths), torch.stack(labels)\n",
    "\n",
    "# Create dataset\n",
    "dataset = ProteinSequenceDataset(sequences, labels, aa_to_idx)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Validation: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Number of batches:\")\n",
    "print(f\"  Train: {len(train_loader)}\")\n",
    "print(f\"  Validation: {len(val_loader)}\")\n",
    "print(f\"  Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Bidirectional LSTM Model\n",
    "\n",
    "### Why Bidirectional?\n",
    "\n",
    "Bidirectional LSTMs process the sequence in both directions:\n",
    "- **Forward LSTM**: Reads sequence left-to-right (N-terminus to C-terminus)\n",
    "- **Backward LSTM**: Reads sequence right-to-left\n",
    "- **Combined**: Captures context from both directions\n",
    "\n",
    "This is powerful for proteins because functional regions can depend on context from both sides!\n",
    "\n",
    "### LSTM Components:\n",
    "\n",
    "1. **Embedding Layer**: Converts amino acid indices to dense vectors\n",
    "2. **LSTM Layers**: Process the sequence while maintaining memory\n",
    "3. **Dropout**: Prevents overfitting\n",
    "4. **Fully Connected**: Maps LSTM output to class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for protein sequence classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, \n",
    "                 num_layers=2, num_classes=3, dropout=0.3):\n",
    "        super(ProteinLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer: converts amino acid indices to dense vectors\n",
    "        # Each amino acid gets a learnable embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0  # Padding token\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        # bidirectional=True means we process sequence in both directions\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # *2 because bidirectional (forward + backward)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, sequences, lengths):\n",
    "        # Embed sequences\n",
    "        embedded = self.embedding(sequences)  # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        # Pack padded sequences for efficient processing\n",
    "        # This tells PyTorch to ignore padding during computation\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the final hidden state from both directions\n",
    "        # hidden shape: (num_layers * 2, batch, hidden_dim)\n",
    "        # We take the last layer's forward and backward hidden states\n",
    "        forward_hidden = hidden[-2, :, :]\n",
    "        backward_hidden = hidden[-1, :, :]\n",
    "        \n",
    "        # Concatenate forward and backward\n",
    "        combined = torch.cat((forward_hidden, backward_hidden), dim=1)\n",
    "        \n",
    "        # Apply dropout and fully connected layer\n",
    "        out = self.dropout(combined)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create model\n",
    "vocab_size = len(aa_to_idx)\n",
    "model = ProteinLSTM(vocab_size=vocab_size, num_classes=3).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, lengths, labels in tqdm(loader, desc=\"Training\"):\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(sequences, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, lengths, labels in loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "Note: We use **gradient clipping** to prevent exploding gradients, which is common in RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Losses\n",
    "ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax1.plot(val_losses, label='Validation Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
    "ax2.plot(val_accs, label='Validation Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, lengths, labels in loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences, lengths)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "# Evaluate\n",
    "test_predictions, test_labels = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    test_labels, test_predictions, \n",
    "    target_names=['Kinase', 'Protease', 'Transporter']\n",
    "))\n",
    "\n",
    "test_accuracy = 100 * np.sum(test_predictions == test_labels) / len(test_labels)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Kinase', 'Protease', 'Transporter'],\n",
    "            yticklabels=['Kinase', 'Protease', 'Transporter'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix - Protein Family Classification')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, name in enumerate(['Kinase', 'Protease', 'Transporter']):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    print(f\"  {name}: {100 * class_correct / class_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Understanding LSTM vs Simple RNN\n",
    "\n",
    "Let's create a simple RNN for comparison to understand why LSTMs are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for comparison with LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_classes=3):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, sequences, lengths):\n",
    "        embedded = self.embedding(sequences)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        \n",
    "        forward_hidden = hidden[-2, :, :]\n",
    "        backward_hidden = hidden[-1, :, :]\n",
    "        combined = torch.cat((forward_hidden, backward_hidden), dim=1)\n",
    "        \n",
    "        return self.fc(combined)\n",
    "\n",
    "print(\"\\nKey differences between RNN and LSTM:\")\n",
    "print(\"\\n1. RNN:\")\n",
    "print(\"   - Simple recurrent connections\")\n",
    "print(\"   - Suffers from vanishing/exploding gradients\")\n",
    "print(\"   - Poor at learning long-term dependencies\")\n",
    "print(\"\\n2. LSTM:\")\n",
    "print(\"   - Has memory cells and gates (input, forget, output)\")\n",
    "print(\"   - Better gradient flow through time\")\n",
    "print(\"   - Can learn long-range dependencies\")\n",
    "print(\"   - More parameters but more powerful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. \u2705 **Encoded protein sequences** using integer encoding and embeddings\n",
    "2. \u2705 **Built a bidirectional LSTM** to capture context from both directions\n",
    "3. \u2705 **Handled variable-length sequences** efficiently with padding and packing\n",
    "4. \u2705 **Trained the model** with proper regularization (dropout, gradient clipping)\n",
    "5. \u2705 **Evaluated multi-class classification** performance\n",
    "\n",
    "### Why LSTMs for Protein Sequences?\n",
    "\n",
    "- **Sequential nature**: Proteins have sequential dependencies\n",
    "- **Variable length**: Proteins vary greatly in length (50-30,000 amino acids)\n",
    "- **Long-range interactions**: Functional sites can be far apart in sequence\n",
    "- **Bidirectional context**: N-terminal and C-terminal regions both matter\n",
    "\n",
    "### Advanced Techniques (Next Steps):\n",
    "\n",
    "- **Attention mechanisms**: Focus on important regions\n",
    "- **Transfer learning**: Use pre-trained protein language models (ESM, ProtTrans)\n",
    "- **Multi-task learning**: Predict multiple properties simultaneously\n",
    "- **GRU**: Alternative to LSTM with fewer parameters\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- Protein function prediction\n",
    "- Subcellular localization\n",
    "- Post-translational modification sites\n",
    "- Protein-protein interaction prediction\n",
    "- Drug target identification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}