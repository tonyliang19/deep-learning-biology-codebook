{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Welcome to Chapter 2! Here we'll explore CNNs, the powerhouse architecture for image analysis and a key tool in biological image processing.\n",
    "\n",
    "## \ud83d\udcda Table of Contents\n",
    "1. [Why Convolutions?](#why-convolutions)\n",
    "2. [Understanding Convolution Operation](#convolution-op)\n",
    "3. [CNN Components](#cnn-components)\n",
    "4. [Building Your First CNN](#first-cnn)\n",
    "5. [Popular CNN Architectures](#architectures)\n",
    "6. [Transfer Learning](#transfer-learning)\n",
    "7. [Biology Application: Cell Image Classification](#biology-app)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'GPU available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Convolutions? <a id=\"why-convolutions\"></a>\n",
    "\n",
    "### Problems with Fully Connected Networks for Images\n",
    "\n",
    "Imagine a small 28\u00d728 grayscale image (like MNIST digits):\n",
    "- Input size: 28 \u00d7 28 = 784 pixels\n",
    "- Hidden layer: 1000 neurons\n",
    "- **Parameters needed**: 784 \u00d7 1000 = 784,000!\n",
    "\n",
    "For a color image (224\u00d7224\u00d73):\n",
    "- Input size: 224 \u00d7 224 \u00d7 3 = 150,528 pixels\n",
    "- Hidden layer: 1000 neurons\n",
    "- **Parameters needed**: 150,528 \u00d7 1000 = 150 million!\n",
    "\n",
    "### Key Insights for Images\n",
    "\n",
    "1. **Local Connectivity**: Nearby pixels are more related than distant ones\n",
    "2. **Translation Invariance**: A cat is a cat whether it's in the top-left or bottom-right\n",
    "3. **Hierarchical Features**: Low-level features (edges) combine to form high-level features (objects)\n",
    "\n",
    "### Solution: Convolutional Layers\n",
    "\n",
    "Convolutions address all three issues:\n",
    "- **Local receptive fields**: Each neuron only looks at a small region\n",
    "- **Shared weights**: Same filters scan across the entire image\n",
    "- **Hierarchical learning**: Stack layers to build complexity\n",
    "\n",
    "Let's visualize a convolution operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_convolution():\n",
    "    \"\"\"Visualize a simple 2D convolution operation.\"\"\"\n",
    "    \n",
    "    # Create a simple input (5x5)\n",
    "    input_img = np.array([\n",
    "        [1, 1, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 0, 1, 1, 0],\n",
    "        [0, 1, 1, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # Edge detection kernel (3x3)\n",
    "    kernel = np.array([\n",
    "        [-1, -1, -1],\n",
    "        [-1,  8, -1],\n",
    "        [-1, -1, -1]\n",
    "    ])\n",
    "    \n",
    "    # Perform convolution manually\n",
    "    output_size = input_img.shape[0] - kernel.shape[0] + 1\n",
    "    output = np.zeros((output_size, output_size))\n",
    "    \n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            # Extract region\n",
    "            region = input_img[i:i+3, j:j+3]\n",
    "            # Apply kernel\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Input\n",
    "    im1 = axes[0].imshow(input_img, cmap='gray', interpolation='nearest')\n",
    "    axes[0].set_title('Input Image (5\u00d75)', fontsize=13, weight='bold')\n",
    "    axes[0].grid(True, which='both', color='red', linewidth=0.5, alpha=0.3)\n",
    "    axes[0].set_xticks(np.arange(-0.5, 5, 1), minor=True)\n",
    "    axes[0].set_yticks(np.arange(-0.5, 5, 1), minor=True)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axes[0].text(j, i, str(int(input_img[i, j])), \n",
    "                        ha='center', va='center', color='red', fontsize=11, weight='bold')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Kernel\n",
    "    im2 = axes[1].imshow(kernel, cmap='RdBu', interpolation='nearest', vmin=-8, vmax=8)\n",
    "    axes[1].set_title('Edge Detection Kernel (3\u00d73)', fontsize=13, weight='bold')\n",
    "    axes[1].grid(True, which='both', color='black', linewidth=0.5, alpha=0.3)\n",
    "    axes[1].set_xticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    axes[1].set_yticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axes[1].text(j, i, str(int(kernel[i, j])), \n",
    "                        ha='center', va='center', color='black', fontsize=11, weight='bold')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Output\n",
    "    im3 = axes[2].imshow(output, cmap='viridis', interpolation='nearest')\n",
    "    axes[2].set_title('Output Feature Map (3\u00d73)', fontsize=13, weight='bold')\n",
    "    axes[2].grid(True, which='both', color='white', linewidth=0.5, alpha=0.3)\n",
    "    axes[2].set_xticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    axes[2].set_yticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            axes[2].text(j, i, f'{output[i, j]:.0f}', \n",
    "                        ha='center', va='center', color='white', fontsize=11, weight='bold')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\ud83d\udcca Convolution Operation:')\n",
    "    print('Input (5\u00d75) * Kernel (3\u00d73) = Output (3\u00d73)')\n",
    "    print('\\nOutput size formula: (input_size - kernel_size + 1)')\n",
    "    print('In this case: (5 - 3 + 1) = 3')\n",
    "\n",
    "visualize_convolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Convolution Operation <a id=\"convolution-op\"></a>\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For a 2D input $I$ and kernel $K$, the convolution at position $(i, j)$ is:\n",
    "\n",
    "$$S(i, j) = (I * K)(i, j) = \\sum_{m} \\sum_{n} I(i+m, j+n) \\cdot K(m, n)$$\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "1. **Kernel Size**: Size of the filter (e.g., 3\u00d73, 5\u00d75)\n",
    "2. **Stride**: Step size when sliding kernel (default: 1)\n",
    "3. **Padding**: Add zeros around input to control output size\n",
    "4. **Dilation**: Spacing between kernel elements\n",
    "\n",
    "### Output Size Calculation\n",
    "\n",
    "$$O = \\frac{W - K + 2P}{S} + 1$$\n",
    "\n",
    "where:\n",
    "- $O$ = output size\n",
    "- $W$ = input size\n",
    "- $K$ = kernel size\n",
    "- $P$ = padding\n",
    "- $S$ = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_conv_parameters():\n",
    "    \"\"\"Demonstrate effect of different convolution parameters.\"\"\"\n",
    "    \n",
    "    # Create a sample input\n",
    "    x = torch.randn(1, 1, 7, 7)  # batch=1, channels=1, height=7, width=7\n",
    "    \n",
    "    print('Input shape:', x.shape)\n",
    "    print('Format: (batch_size, channels, height, width)\\n')\n",
    "    \n",
    "    # Different configurations\n",
    "    configs = [\n",
    "        {'kernel_size': 3, 'stride': 1, 'padding': 0, 'name': 'Default'},\n",
    "        {'kernel_size': 3, 'stride': 2, 'padding': 0, 'name': 'Stride=2'},\n",
    "        {'kernel_size': 3, 'stride': 1, 'padding': 1, 'name': 'Padding=1 (same)'},\n",
    "        {'kernel_size': 5, 'stride': 1, 'padding': 0, 'name': 'Kernel=5\u00d75'},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for config in configs:\n",
    "        conv = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                        kernel_size=config['kernel_size'],\n",
    "                        stride=config['stride'],\n",
    "                        padding=config['padding'])\n",
    "        output = conv(x)\n",
    "        \n",
    "        # Calculate output size using formula\n",
    "        calc_size = int((7 - config['kernel_size'] + 2*config['padding']) / config['stride'] + 1)\n",
    "        \n",
    "        result = f\"{config['name']:20s} | Output: {output.shape[2]}\u00d7{output.shape[3]} (calculated: {calc_size}\u00d7{calc_size})\"\n",
    "        results.append(result)\n",
    "        print(result)\n",
    "    \n",
    "    print('\\n\ud83d\udca1 Key Insight:')\n",
    "    print('  - Stride > 1: Reduces spatial dimensions (downsampling)')\n",
    "    print('  - Padding = (kernel_size-1)/2: Maintains input size (\"same\" padding)')\n",
    "    print('  - Larger kernels: See more context but fewer parameters')\n",
    "\n",
    "demonstrate_conv_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN Components <a id=\"cnn-components\"></a>\n",
    "\n",
    "A typical CNN consists of:\n",
    "\n",
    "### 1. Convolutional Layers\n",
    "- Learn spatial hierarchies of features\n",
    "- Share weights across spatial locations\n",
    "- Each filter detects a specific pattern\n",
    "\n",
    "### 2. Activation Functions\n",
    "- Usually ReLU: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "- Introduces non-linearity\n",
    "\n",
    "### 3. Pooling Layers\n",
    "- Reduce spatial dimensions\n",
    "- Provide translation invariance\n",
    "- Types: Max pooling, Average pooling\n",
    "\n",
    "### 4. Fully Connected Layers\n",
    "- Final classification\n",
    "- Combine all features\n",
    "\n",
    "### 5. Dropout (Regularization)\n",
    "- Randomly drop neurons during training\n",
    "- Prevents overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pooling():\n",
    "    \"\"\"Visualize max pooling operation.\"\"\"\n",
    "    \n",
    "    # Create input\n",
    "    input_data = np.array([\n",
    "        [1, 3, 2, 4],\n",
    "        [5, 6, 7, 8],\n",
    "        [9, 2, 3, 1],\n",
    "        [0, 4, 5, 2]\n",
    "    ])\n",
    "    \n",
    "    # Max pooling 2x2\n",
    "    output = np.zeros((2, 2))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            region = input_data[i*2:(i+1)*2, j*2:(j+1)*2]\n",
    "            output[i, j] = np.max(region)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Input\n",
    "    im1 = axes[0].imshow(input_data, cmap='YlOrRd', interpolation='nearest')\n",
    "    axes[0].set_title('Input (4\u00d74)', fontsize=13, weight='bold')\n",
    "    axes[0].grid(True, which='both', color='black', linewidth=2)\n",
    "    axes[0].set_xticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    axes[0].set_yticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    \n",
    "    # Add pooling windows\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            rect = plt.Rectangle((j*2-0.5, i*2-0.5), 2, 2, \n",
    "                                fill=False, edgecolor='blue', linewidth=3)\n",
    "            axes[0].add_patch(rect)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[0].text(j, i, str(int(input_data[i, j])), \n",
    "                        ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Output\n",
    "    im2 = axes[1].imshow(output, cmap='YlOrRd', interpolation='nearest')\n",
    "    axes[1].set_title('Max Pooled Output (2\u00d72)', fontsize=13, weight='bold')\n",
    "    axes[1].grid(True, which='both', color='black', linewidth=2)\n",
    "    axes[1].set_xticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "    axes[1].set_yticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[1].text(j, i, str(int(output[i, j])), \n",
    "                        ha='center', va='center', color='black', fontsize=14, weight='bold')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\ud83d\udcca Max Pooling (2\u00d72, stride=2):')\n",
    "    print('Takes maximum value from each 2\u00d72 region')\n",
    "    print('Reduces spatial dimensions by half')\n",
    "    print('Provides translation invariance')\n",
    "\n",
    "visualize_pooling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}