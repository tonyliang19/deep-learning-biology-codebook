{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Welcome to Chapter 2! \ud83d\uddbc\ufe0f\n",
    "\n",
    "In this chapter, you'll discover why CNNs are the go-to architecture for anything involving images or spatial data. CNNs power everything from facial recognition on your phone to medical image analysis in hospitals!\n",
    "\n",
    "**What you'll learn:**\n",
    "- Why regular neural networks struggle with images\n",
    "- How convolutions work (it's like using a magnifying glass to scan an image)\n",
    "- Build CNNs that can recognize patterns in images\n",
    "- Apply CNNs to biological images (like classifying cell types)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Chapter 1 (Neural Networks Basics) - we'll build on those concepts\n",
    "- Basic understanding of images as grids of pixels\n",
    "- Familiarity with matrix operations (we'll explain as we go)\n",
    "\n",
    "## \ud83d\udcda Table of Contents\n",
    "1. [Why Convolutions?](#why-convolutions)\n",
    "2. [Understanding Convolution Operation](#convolution-op)\n",
    "3. [CNN Components](#cnn-components)\n",
    "4. [Building Your First CNN](#first-cnn)\n",
    "5. [Popular CNN Architectures](#architectures)\n",
    "6. [Transfer Learning](#transfer-learning)\n",
    "7. [Biology Application: Cell Image Classification](#biology-app)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'GPU available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Convolutions? <a id=\"why-convolutions\"></a>\n",
    "\n",
    "### The Big Problem with Regular Neural Networks for Images\n",
    "\n",
    "Let's understand why we need something special for images:\n",
    "\n",
    "**Example 1: A tiny MNIST digit (28\u00d728 pixels, grayscale)**\n",
    "- Total inputs: 28 \u00d7 28 = 784 pixels\n",
    "- If we want 1000 neurons in first layer: 784 \u00d7 1000 = **784,000 parameters** (weights)\n",
    "- That's a lot, but manageable...\n",
    "\n",
    "**Example 2: A small color photo (224\u00d7224 pixels, RGB)**\n",
    "- Total inputs: 224 \u00d7 224 \u00d7 3 (colors) = 150,528 pixels\n",
    "- With 1000 neurons: 150,528 \u00d7 1000 = **150,528,000 parameters!**\n",
    "- This is getting out of control...\n",
    "\n",
    "**Example 3: A high-resolution medical image (1024\u00d71024 pixels, RGB)**\n",
    "- Total inputs: 1024 \u00d7 1024 \u00d7 3 = 3,145,728 pixels\n",
    "- With 1000 neurons: **3.1 BILLION parameters!**\n",
    "- Your computer would run out of memory! \ud83d\udca5\n",
    "\n",
    "### Why So Many Parameters is Bad\n",
    "\n",
    "1. **Too much memory**: Your computer can't store all those weights\n",
    "2. **Too slow**: Training takes forever (or never finishes)\n",
    "3. **Overfitting**: The model memorizes training images instead of learning patterns\n",
    "4. **Ignores structure**: A regular network doesn't know that nearby pixels are related\n",
    "\n",
    "### The CNN Solution: Three Key Ideas\n",
    "\n",
    "CNNs are so much better because they use these insights:\n",
    "\n",
    "**1. Local Connectivity (nearby pixels matter more)**\n",
    "- Analogy: When looking at a face, the eyes, nose, and mouth near each other matter more than a random eye and a distant toe\n",
    "- Solution: Each neuron only connects to a small patch of the image (e.g., 3\u00d73 or 5\u00d75 pixels)\n",
    "- Result: Fewer parameters! Instead of 150 million, maybe just thousands\n",
    "\n",
    "**2. Parameter Sharing (reuse same detector everywhere)**\n",
    "- Analogy: If you have a \"cat detector,\" it should work whether the cat is in the top-left or bottom-right of the image\n",
    "- Solution: Use the same filter (set of weights) across the entire image\n",
    "- Result: Even fewer parameters! The same 3\u00d73 filter is used everywhere\n",
    "\n",
    "**3. Translation Invariance (position doesn't matter)**\n",
    "- Analogy: A cat is still a cat whether it's in the center or corner of a photo\n",
    "- Solution: Convolution operation naturally handles this\n",
    "- Result: Model generalizes better to new images\n",
    "\n",
    "### Real-World Analogy\n",
    "\n",
    "Think of reading a book:\n",
    "- **Regular Neural Network**: Memorizing the exact position of every word on every page (impossible!)\n",
    "- **CNN**: Learning patterns (letter shapes, word structures) that work anywhere on the page (practical!)\n",
    "\n",
    "Let's visualize what convolution actually does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_convolution():\n",
    "    \"\"\"Visualize a simple 2D convolution operation.\"\"\"\n",
    "    \n",
    "    # Create a simple input (5x5)\n",
    "    input_img = np.array([\n",
    "        [1, 1, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 0, 1, 1, 0],\n",
    "        [0, 1, 1, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # Edge detection kernel (3x3)\n",
    "    kernel = np.array([\n",
    "        [-1, -1, -1],\n",
    "        [-1,  8, -1],\n",
    "        [-1, -1, -1]\n",
    "    ])\n",
    "    \n",
    "    # Perform convolution manually\n",
    "    output_size = input_img.shape[0] - kernel.shape[0] + 1\n",
    "    output = np.zeros((output_size, output_size))\n",
    "    \n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            # Extract region\n",
    "            region = input_img[i:i+3, j:j+3]\n",
    "            # Apply kernel\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Input\n",
    "    im1 = axes[0].imshow(input_img, cmap='gray', interpolation='nearest')\n",
    "    axes[0].set_title('Input Image (5\u00d75)', fontsize=13, weight='bold')\n",
    "    axes[0].grid(True, which='both', color='red', linewidth=0.5, alpha=0.3)\n",
    "    axes[0].set_xticks(np.arange(-0.5, 5, 1), minor=True)\n",
    "    axes[0].set_yticks(np.arange(-0.5, 5, 1), minor=True)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axes[0].text(j, i, str(int(input_img[i, j])), \n",
    "                        ha='center', va='center', color='red', fontsize=11, weight='bold')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Kernel\n",
    "    im2 = axes[1].imshow(kernel, cmap='RdBu', interpolation='nearest', vmin=-8, vmax=8)\n",
    "    axes[1].set_title('Edge Detection Kernel (3\u00d73)', fontsize=13, weight='bold')\n",
    "    axes[1].grid(True, which='both', color='black', linewidth=0.5, alpha=0.3)\n",
    "    axes[1].set_xticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    axes[1].set_yticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            axes[1].text(j, i, str(int(kernel[i, j])), \n",
    "                        ha='center', va='center', color='black', fontsize=11, weight='bold')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Output\n",
    "    im3 = axes[2].imshow(output, cmap='viridis', interpolation='nearest')\n",
    "    axes[2].set_title('Output Feature Map (3\u00d73)', fontsize=13, weight='bold')\n",
    "    axes[2].grid(True, which='both', color='white', linewidth=0.5, alpha=0.3)\n",
    "    axes[2].set_xticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    axes[2].set_yticks(np.arange(-0.5, 3, 1), minor=True)\n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            axes[2].text(j, i, f'{output[i, j]:.0f}', \n",
    "                        ha='center', va='center', color='white', fontsize=11, weight='bold')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\ud83d\udcca Convolution Operation:')\n",
    "    print('Input (5\u00d75) * Kernel (3\u00d73) = Output (3\u00d73)')\n",
    "    print('\\nOutput size formula: (input_size - kernel_size + 1)')\n",
    "    print('In this case: (5 - 3 + 1) = 3')\n",
    "\n",
    "visualize_convolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Convolution Operation <a id=\"convolution-op\"></a>\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For a 2D input $I$ and kernel $K$, the convolution at position $(i, j)$ is:\n",
    "\n",
    "$$S(i, j) = (I * K)(i, j) = \\sum_{m} \\sum_{n} I(i+m, j+n) \\cdot K(m, n)$$\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "1. **Kernel Size**: Size of the filter (e.g., 3\u00d73, 5\u00d75)\n",
    "2. **Stride**: Step size when sliding kernel (default: 1)\n",
    "3. **Padding**: Add zeros around input to control output size\n",
    "4. **Dilation**: Spacing between kernel elements\n",
    "\n",
    "### Output Size Calculation\n",
    "\n",
    "$$O = \\frac{W - K + 2P}{S} + 1$$\n",
    "\n",
    "where:\n",
    "- $O$ = output size\n",
    "- $W$ = input size\n",
    "- $K$ = kernel size\n",
    "- $P$ = padding\n",
    "- $S$ = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_conv_parameters():\n",
    "    \"\"\"Demonstrate effect of different convolution parameters.\"\"\"\n",
    "    \n",
    "    # Create a sample input\n",
    "    x = torch.randn(1, 1, 7, 7)  # batch=1, channels=1, height=7, width=7\n",
    "    \n",
    "    print('Input shape:', x.shape)\n",
    "    print('Format: (batch_size, channels, height, width)\\n')\n",
    "    \n",
    "    # Different configurations\n",
    "    configs = [\n",
    "        {'kernel_size': 3, 'stride': 1, 'padding': 0, 'name': 'Default'},\n",
    "        {'kernel_size': 3, 'stride': 2, 'padding': 0, 'name': 'Stride=2'},\n",
    "        {'kernel_size': 3, 'stride': 1, 'padding': 1, 'name': 'Padding=1 (same)'},\n",
    "        {'kernel_size': 5, 'stride': 1, 'padding': 0, 'name': 'Kernel=5\u00d75'},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for config in configs:\n",
    "        conv = nn.Conv2d(in_channels=1, out_channels=1, \n",
    "                        kernel_size=config['kernel_size'],\n",
    "                        stride=config['stride'],\n",
    "                        padding=config['padding'])\n",
    "        output = conv(x)\n",
    "        \n",
    "        # Calculate output size using formula\n",
    "        calc_size = int((7 - config['kernel_size'] + 2*config['padding']) / config['stride'] + 1)\n",
    "        \n",
    "        result = f\"{config['name']:20s} | Output: {output.shape[2]}\u00d7{output.shape[3]} (calculated: {calc_size}\u00d7{calc_size})\"\n",
    "        results.append(result)\n",
    "        print(result)\n",
    "    \n",
    "    print('\\n\ud83d\udca1 Key Insight:')\n",
    "    print('  - Stride > 1: Reduces spatial dimensions (downsampling)')\n",
    "    print('  - Padding = (kernel_size-1)/2: Maintains input size (\"same\" padding)')\n",
    "    print('  - Larger kernels: See more context but fewer parameters')\n",
    "\n",
    "demonstrate_conv_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN Components <a id=\"cnn-components\"></a>\n",
    "\n",
    "A typical CNN consists of:\n",
    "\n",
    "### 1. Convolutional Layers\n",
    "- Learn spatial hierarchies of features\n",
    "- Share weights across spatial locations\n",
    "- Each filter detects a specific pattern\n",
    "\n",
    "### 2. Activation Functions\n",
    "- Usually ReLU: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "- Introduces non-linearity\n",
    "\n",
    "### 3. Pooling Layers\n",
    "- Reduce spatial dimensions\n",
    "- Provide translation invariance\n",
    "- Types: Max pooling, Average pooling\n",
    "\n",
    "### 4. Fully Connected Layers\n",
    "- Final classification\n",
    "- Combine all features\n",
    "\n",
    "### 5. Dropout (Regularization)\n",
    "- Randomly drop neurons during training\n",
    "- Prevents overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pooling():\n",
    "    \"\"\"Visualize max pooling operation.\"\"\"\n",
    "    \n",
    "    # Create input\n",
    "    input_data = np.array([\n",
    "        [1, 3, 2, 4],\n",
    "        [5, 6, 7, 8],\n",
    "        [9, 2, 3, 1],\n",
    "        [0, 4, 5, 2]\n",
    "    ])\n",
    "    \n",
    "    # Max pooling 2x2\n",
    "    output = np.zeros((2, 2))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            region = input_data[i*2:(i+1)*2, j*2:(j+1)*2]\n",
    "            output[i, j] = np.max(region)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Input\n",
    "    im1 = axes[0].imshow(input_data, cmap='YlOrRd', interpolation='nearest')\n",
    "    axes[0].set_title('Input (4\u00d74)', fontsize=13, weight='bold')\n",
    "    axes[0].grid(True, which='both', color='black', linewidth=2)\n",
    "    axes[0].set_xticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    axes[0].set_yticks(np.arange(-0.5, 4, 1), minor=True)\n",
    "    \n",
    "    # Add pooling windows\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            rect = plt.Rectangle((j*2-0.5, i*2-0.5), 2, 2, \n",
    "                                fill=False, edgecolor='blue', linewidth=3)\n",
    "            axes[0].add_patch(rect)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[0].text(j, i, str(int(input_data[i, j])), \n",
    "                        ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Output\n",
    "    im2 = axes[1].imshow(output, cmap='YlOrRd', interpolation='nearest')\n",
    "    axes[1].set_title('Max Pooled Output (2\u00d72)', fontsize=13, weight='bold')\n",
    "    axes[1].grid(True, which='both', color='black', linewidth=2)\n",
    "    axes[1].set_xticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "    axes[1].set_yticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[1].text(j, i, str(int(output[i, j])), \n",
    "                        ha='center', va='center', color='black', fontsize=14, weight='bold')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\ud83d\udcca Max Pooling (2\u00d72, stride=2):')\n",
    "    print('Takes maximum value from each 2\u00d72 region')\n",
    "    print('Reduces spatial dimensions by half')\n",
    "    print('Provides translation invariance')\n",
    "\n",
    "visualize_pooling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}