{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Welcome to Chapter 2! \ud83d\uddbc\ufe0f\n",
        "\n",
        "In this chapter, you'll discover why CNNs are the go-to architecture for anything involving images or spatial data. CNNs power everything from facial recognition on your phone to medical image analysis in hospitals!\n",
        "\n",
        "**What you'll learn:**\n",
        "- Why regular neural networks struggle with images\n",
        "- How convolutions work (it's like using a magnifying glass to scan an image)\n",
        "- Build CNNs that can recognize patterns in images\n",
        "- Apply CNNs to biological images (like classifying cell types)\n",
        "\n",
        "**Prerequisites:**\n",
        "- Chapter 1 (Neural Networks Basics) - we'll build on those concepts\n",
        "- Basic understanding of images as grids of pixels\n",
        "- Familiarity with matrix operations (we'll explain as we go)\n",
        "\n",
        "## \ud83d\udcda Table of Contents\n",
        "1. [Why Convolutions?](#why-convolutions)\n",
        "2. [Understanding Convolution Operation](#convolution-op)\n",
        "3. [CNN Components](#cnn-components)\n",
        "4. [Building Your First CNN](#first-cnn)\n",
        "5. [Popular CNN Architectures](#architectures)\n",
        "6. [Transfer Learning](#transfer-learning)\n",
        "7. [Biology Application: Cell Image Classification](#biology-app)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print('Libraries imported successfully!')\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'GPU available: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Why Convolutions? <a id=\"why-convolutions\"></a>\n\n### The Big Problem with Regular Neural Networks for Images\n\nLet's understand why we need something special for images:\n\n**Example 1: A tiny MNIST digit (28\u00d728 pixels, grayscale)**\n- Total inputs: 28 \u00d7 28 = 784 pixels\n- If we want 1000 neurons in first layer: 784 \u00d7 1000 = **784,000 parameters** (weights)\n- That's a lot, but manageable...\n\n**Example 2: A small color photo (224\u00d7224 pixels, RGB)**\n- Total inputs: 224 \u00d7 224 \u00d7 3 (colors) = 150,528 pixels\n- With 1000 neurons: 150,528 \u00d7 1000 = **150,528,000 parameters!**\n- This is getting out of control...\n\n**Example 3: A high-resolution medical image (1024\u00d71024 pixels, RGB)**\n- Total inputs: 1024 \u00d7 1024 \u00d7 3 = 3,145,728 pixels\n- With 1000 neurons: **3.1 BILLION parameters!**\n- Your computer would run out of memory! \ud83d\udca5\n\n### Why So Many Parameters is Bad\n\n1. **Too much memory**: Your computer can't store all those weights\n2. **Too slow**: Training takes forever (or never finishes)\n3. **Overfitting**: The model memorizes training images instead of learning patterns\n4. **Ignores structure**: A regular network doesn't know that nearby pixels are related\n\n### The CNN Solution: Three Key Ideas\n\nCNNs are so much better because they use these insights:\n\n**1. Local Connectivity (nearby pixels matter more)**\n- Analogy: When looking at a face, the eyes, nose, and mouth near each other matter more than a random eye and a distant toe\n- Solution: Each neuron only connects to a small patch of the image (e.g., 3\u00d73 or 5\u00d75 pixels)\n- Result: Fewer parameters! Instead of 150 million, maybe just thousands\n\n**2. Parameter Sharing (reuse same detector everywhere)**\n- Analogy: If you have a \"cat detector,\" it should work whether the cat is in the top-left or bottom-right of the image\n- Solution: Use the same filter (set of weights) across the entire image\n- Result: Even fewer parameters! The same 3\u00d73 filter is used everywhere\n\n**3. Translation Invariance (position doesn't matter)**\n- Analogy: A cat is still a cat whether it's in the center or corner of a photo\n- Solution: Convolution operation naturally handles this\n- Result: Model generalizes better to new images\n\n### Real-World Analogy\n\nThink of reading a book:\n- **Regular Neural Network**: Memorizing the exact position of every word on every page (impossible!)\n- **CNN**: Learning patterns (letter shapes, word structures) that work anywhere on the page (practical!)\n\nLet's visualize what convolution actually does:\n\n\n### Connections to Signal Processing and Classical Methods\n\n#### Convolution in Mathematics and Signal Processing\n\nThe convolution operation used in CNNs is borrowed directly from **signal processing** and **classical applied mathematics** [@strang2016introduction]:\n\nIn continuous form:\n$$(f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) d\\tau$$\n\nIn discrete form (what CNNs use):\n$$(I * K)(i, j) = \\sum_{m} \\sum_{n} I(i-m, j-n) \\cdot K(m, n)$$\n\n**Historical context:** Convolutions have been used in signal processing since the 1960s for:\n- Audio filtering (removing noise from sound)\n- Image processing (edge detection, blurring, sharpening)\n- Time series analysis (smoothing, trend detection)\n\n**What CNNs innovate:** Instead of using hand-designed filters (like edge detectors), CNNs **learn the optimal filters** from data through backpropagation!\n\n#### Translation Equivariance: A Mathematical Property\n\nCNNs have a beautiful mathematical property called **translation equivariance**:\n\nIf input $I$ is shifted by vector $v$, the output is also shifted by $v$:\n$$\\text{CNN}(I \\text{ shifted by } v) = \\text{CNN}(I) \\text{ shifted by } v$$\n\n**Why this matters:**\n- A cat is still a cat whether it appears in the top-left or bottom-right of an image\n- This property is built into the architecture, not learned from data\n- Classical fully-connected networks must learn this property separately for each location (inefficient!)\n\n#### Comparison with Feature Engineering\n\n**Classical computer vision (pre-CNN)** [@lecun1998gradient]:\n1. Hand-design filters (e.g., Gabor filters, SIFT features, HOG features)\n2. Apply filters to extract features\n3. Use simple classifier (SVM, logistic regression) on extracted features\n\n**CNN approach:**\n1. Learn filters automatically from training data\n2. Hierarchically compose simple filters into complex ones\n3. Integrate feature extraction and classification (end-to-end learning)\n\n**Statistical perspective:** CNNs perform **automatic feature selection** within a constrained hypothesis space (convolutional structure). This is similar to regularization in classical statistics - we're constraining the model to reduce overfitting, but unlike $L_1$ or $L_2$ regularization, we're using architectural constraints (weight sharing, local connectivity).\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_convolution():\n",
        "    \"\"\"Visualize a simple 2D convolution operation.\"\"\"\n",
        "    \n",
        "    # Create a simple input (5x5)\n",
        "    input_img = np.array([\n",
        "        [1, 1, 1, 0, 0],\n",
        "        [0, 1, 1, 1, 0],\n",
        "        [0, 0, 1, 1, 1],\n",
        "        [0, 0, 1, 1, 0],\n",
        "        [0, 1, 1, 0, 0]\n",
        "    ])\n",
        "    \n",
        "    # Edge detection kernel (3x3)\n",
        "    kernel = np.array([\n",
        "        [-1, -1, -1],\n",
        "        [-1,  8, -1],\n",
        "        [-1, -1, -1]\n",
        "    ])\n",
        "    \n",
        "    # Perform convolution manually\n",
        "    output_size = input_img.shape[0] - kernel.shape[0] + 1\n",
        "    output = np.zeros((output_size, output_size))\n",
        "    \n",
        "    for i in range(output_size):\n",
        "        for j in range(output_size):\n",
        "            # Extract region\n",
        "            region = input_img[i:i+3, j:j+3]\n",
        "            # Apply kernel\n",
        "            output[i, j] = np.sum(region * kernel)\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    # Input\n",
        "    im1 = axes[0].imshow(input_img, cmap='gray', interpolation='nearest')\n",
        "    axes[0].set_title('Input Image (5\u00d75)', fontsize=13, weight='bold')\n",
        "    axes[0].grid(True, which='both', color='red', linewidth=0.5, alpha=0.3)\n",
        "    axes[0].set_xticks(np.arange(-0.5, 5, 1), minor=True)\n",
        "    axes[0].set_yticks(np.arange(-0.5, 5, 1), minor=True)\n",
        "    for i in range(5):\n",
        "        for j in range(5):\n",
        "            axes[0].text(j, i, str(int(input_img[i, j])), \n",
        "                        ha='center', va='center', color='red', fontsize=11, weight='bold')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "    \n",
        "    # Kernel\n",
        "    im2 = axes[1].imshow(kernel, cmap='RdBu', interpolation='nearest', vmin=-8, vmax=8)\n",
        "    axes[1].set_title('Edge Detection Kernel (3\u00d73)', fontsize=13, weight='bold')\n",
        "    axes[1].grid(True, which='both', color='black', linewidth=0.5, alpha=0.3)\n",
        "    axes[1].set_xticks(np.arange(-0.5, 3, 1), minor=True)\n",
        "    axes[1].set_yticks(np.arange(-0.5, 3, 1), minor=True)\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            axes[1].text(j, i, str(int(kernel[i, j])), \n",
        "                        ha='center', va='center', color='black', fontsize=11, weight='bold')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "    \n",
        "    # Output\n",
        "    im3 = axes[2].imshow(output, cmap='viridis', interpolation='nearest')\n",
        "    axes[2].set_title('Output Feature Map (3\u00d73)', fontsize=13, weight='bold')\n",
        "    axes[2].grid(True, which='both', color='white', linewidth=0.5, alpha=0.3)\n",
        "    axes[2].set_xticks(np.arange(-0.5, 3, 1), minor=True)\n",
        "    axes[2].set_yticks(np.arange(-0.5, 3, 1), minor=True)\n",
        "    for i in range(output_size):\n",
        "        for j in range(output_size):\n",
        "            axes[2].text(j, i, f'{output[i, j]:.0f}', \n",
        "                        ha='center', va='center', color='white', fontsize=11, weight='bold')\n",
        "    plt.colorbar(im3, ax=axes[2])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\n\ud83d\udcca Convolution Operation:')\n",
        "    print('Input (5\u00d75) * Kernel (3\u00d73) = Output (3\u00d73)')\n",
        "    print('\\nOutput size formula: (input_size - kernel_size + 1)')\n",
        "    print('In this case: (5 - 3 + 1) = 3')\n",
        "\n",
        "visualize_convolution()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Understanding Convolution Operation <a id=\"convolution-op\"></a>\n\n### Mathematical Definition\n\nFor a 2D input $I$ and kernel $K$, the convolution at position $(i, j)$ is:\n\n$$S(i, j) = (I * K)(i, j) = \\sum_{m} \\sum_{n} I(i+m, j+n) \\cdot K(m, n)$$\n\n### Key Parameters\n\n1. **Kernel Size**: Size of the filter (e.g., 3\u00d73, 5\u00d75)\n2. **Stride**: Step size when sliding kernel (default: 1)\n3. **Padding**: Add zeros around input to control output size\n4. **Dilation**: Spacing between kernel elements\n\n### Output Size Calculation\n\n$$O = \\frac{W - K + 2P}{S} + 1$$\n\nwhere:\n- $O$ = output size\n- $W$ = input size\n- $K$ = kernel size\n- $P$ = padding\n- $S$ = stride\n\n### Convolution as a Linear Operator\n\nFrom a mathematical perspective, convolution is a **linear operator** - it takes an input and produces an output through a linear transformation, just like matrix multiplication.\n\n#### Connection to Matrix Factorization\n\nInterestingly, the convolution operation can be viewed as a form of **constrained matrix multiplication** where:\n1. **Weight sharing:** The same filter weights are applied at every position\n2. **Sparse connectivity:** Each output depends only on a local region of the input\n\nIn standard matrix multiplication: $Y = WX$ (where $W$ can have any values)\n\nIn convolution: We impose structure on $W$:\n- Many entries in $W$ are zero (sparse, local connectivity)\n- Many entries in $W$ are tied (weight sharing)\n\n**This constraint is a form of regularization**, similar to how in classical statistics we might use:\n- Ridge regression ($L_2$ penalty) to prevent overfitting\n- Lasso regression ($L_1$ penalty) to induce sparsity\n- CNNs use **architectural constraints** (structure of $W$) to prevent overfitting\n\n#### Relationship to Basis Decomposition\n\nA convolution layer with $k$ filters can be thought of as decomposing the input into $k$ different \"views\" or \"basis representations\":\n\n$$\\text{Output} = \\sum_{i=1}^{k} (\\text{Input} * \\text{Filter}_i)$$\n\nThis is analogous to **basis decomposition** in classical analysis (Fourier series, wavelet transforms), but the bases (filters) are learned from data rather than predefined.\n\n**Connection to dictionary learning:** In sparse coding [@hastie2009elements], we learn a dictionary of basis functions. CNNs do something similar but enforce additional structure (locality, hierarchy).\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_conv_parameters():\n",
        "    \"\"\"Demonstrate effect of different convolution parameters.\"\"\"\n",
        "    \n",
        "    # Create a sample input\n",
        "    x = torch.randn(1, 1, 7, 7)  # batch=1, channels=1, height=7, width=7\n",
        "    \n",
        "    print('Input shape:', x.shape)\n",
        "    print('Format: (batch_size, channels, height, width)\\n')\n",
        "    \n",
        "    # Different configurations\n",
        "    configs = [\n",
        "        {'kernel_size': 3, 'stride': 1, 'padding': 0, 'name': 'Default'},\n",
        "        {'kernel_size': 3, 'stride': 2, 'padding': 0, 'name': 'Stride=2'},\n",
        "        {'kernel_size': 3, 'stride': 1, 'padding': 1, 'name': 'Padding=1 (same)'},\n",
        "        {'kernel_size': 5, 'stride': 1, 'padding': 0, 'name': 'Kernel=5\u00d75'},\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    for config in configs:\n",
        "        conv = nn.Conv2d(in_channels=1, out_channels=1, \n",
        "                        kernel_size=config['kernel_size'],\n",
        "                        stride=config['stride'],\n",
        "                        padding=config['padding'])\n",
        "        output = conv(x)\n",
        "        \n",
        "        # Calculate output size using formula\n",
        "        calc_size = int((7 - config['kernel_size'] + 2*config['padding']) / config['stride'] + 1)\n",
        "        \n",
        "        result = f\"{config['name']:20s} | Output: {output.shape[2]}\u00d7{output.shape[3]} (calculated: {calc_size}\u00d7{calc_size})\"\n",
        "        results.append(result)\n",
        "        print(result)\n",
        "    \n",
        "    print('\\n\ud83d\udca1 Key Insight:')\n",
        "    print('  - Stride > 1: Reduces spatial dimensions (downsampling)')\n",
        "    print('  - Padding = (kernel_size-1)/2: Maintains input size (\"same\" padding)')\n",
        "    print('  - Larger kernels: See more context but fewer parameters')\n",
        "\n",
        "demonstrate_conv_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. CNN Components <a id=\"cnn-components\"></a>\n\nA typical CNN consists of:\n\n### 1. Convolutional Layers\n- Learn spatial hierarchies of features\n- Share weights across spatial locations\n- Each filter detects a specific pattern\n\n### 2. Activation Functions\n- Usually ReLU: $\\text{ReLU}(x) = \\max(0, x)$\n- Introduces non-linearity\n\n### 3. Pooling Layers\n- Reduce spatial dimensions\n- Provide translation invariance\n- Types: Max pooling, Average pooling\n\n### 4. Fully Connected Layers\n- Final classification\n- Combine all features\n\n### 5. Dropout (Regularization)\n- Randomly drop neurons during training\n- Prevents overfitting\n\n### Statistical Interpretation of CNN Components\n\n#### Pooling as Downsampling with Robustness\n\n**Max pooling** and **average pooling** serve similar purposes to classical downsampling but with built-in robustness:\n\n**Average pooling:**\n$$\\text{Pool}(x) = \\frac{1}{k^2} \\sum_{i,j \\in \\text{region}} x_{i,j}$$\n- Analogous to **moving average** in time series analysis\n- Reduces variance (smoothing effect)\n- Less sensitive to outliers than max pooling\n\n**Max pooling:**\n$$\\text{Pool}(x) = \\max_{i,j \\in \\text{region}} x_{i,j}$$\n- Provides **translation invariance** (small shifts don't change the max)\n- More robust to noise than average pooling\n- Similar to robust statistics using maximum likelihood\n\n**Statistical insight:** Pooling trades spatial resolution for robustness, similar to how in classical statistics we might use:\n- Binning continuous variables (reduces variance but loses information)\n- Robust estimators (median instead of mean)\n\n#### Dropout as Ensemble Learning\n\n**Dropout** [@srivastava2014dropout] randomly drops neurons during training:\n\nFrom an ensemble learning perspective, dropout trains an exponential number of \"thinned\" networks and averages their predictions. This is similar to:\n\n- **Bagging** in random forests: Train multiple models on subsamples of data\n- **Dropout:** Train multiple sub-networks on subsamples of the architecture\n\n**Mathematical connection to Bayesian inference:** Dropout can be interpreted as approximate Bayesian inference, where we're marginalizing over different network architectures [@bishop2006pattern]. This provides a form of uncertainty estimation.\n\n#### Batch Normalization and Statistical Standardization\n\n**Batch normalization** [@ioffe2015batch] is a direct application of statistical standardization:\n\n$$\\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sqrt{\\sigma^2_{\\text{batch}} + \\epsilon}}$$\n\nThis is exactly **z-score normalization** from statistics! \n\n**Why it helps:**\n- Addresses \"internal covariate shift\" (distribution of layer inputs changes during training)\n- Similar to **feature scaling** in classical ML (normalizing inputs before regression/SVM)\n- Makes optimization easier by keeping activations in a reasonable range\n\nThe learnable parameters ($\\gamma, \\beta$) allow the network to undo this normalization if needed:\n$$y = \\gamma \\hat{x} + \\beta$$\n\nThis is elegant: we provide a good starting point (normalized) but let the network learn the optimal scale and shift.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_pooling():\n",
        "    \"\"\"Visualize max pooling operation.\"\"\"\n",
        "    \n",
        "    # Create input\n",
        "    input_data = np.array([\n",
        "        [1, 3, 2, 4],\n",
        "        [5, 6, 7, 8],\n",
        "        [9, 2, 3, 1],\n",
        "        [0, 4, 5, 2]\n",
        "    ])\n",
        "    \n",
        "    # Max pooling 2x2\n",
        "    output = np.zeros((2, 2))\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            region = input_data[i*2:(i+1)*2, j*2:(j+1)*2]\n",
        "            output[i, j] = np.max(region)\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Input\n",
        "    im1 = axes[0].imshow(input_data, cmap='YlOrRd', interpolation='nearest')\n",
        "    axes[0].set_title('Input (4\u00d74)', fontsize=13, weight='bold')\n",
        "    axes[0].grid(True, which='both', color='black', linewidth=2)\n",
        "    axes[0].set_xticks(np.arange(-0.5, 4, 1), minor=True)\n",
        "    axes[0].set_yticks(np.arange(-0.5, 4, 1), minor=True)\n",
        "    \n",
        "    # Add pooling windows\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            rect = plt.Rectangle((j*2-0.5, i*2-0.5), 2, 2, \n",
        "                                fill=False, edgecolor='blue', linewidth=3)\n",
        "            axes[0].add_patch(rect)\n",
        "    \n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axes[0].text(j, i, str(int(input_data[i, j])), \n",
        "                        ha='center', va='center', color='black', fontsize=12, weight='bold')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "    \n",
        "    # Output\n",
        "    im2 = axes[1].imshow(output, cmap='YlOrRd', interpolation='nearest')\n",
        "    axes[1].set_title('Max Pooled Output (2\u00d72)', fontsize=13, weight='bold')\n",
        "    axes[1].grid(True, which='both', color='black', linewidth=2)\n",
        "    axes[1].set_xticks(np.arange(-0.5, 2, 1), minor=True)\n",
        "    axes[1].set_yticks(np.arange(-0.5, 2, 1), minor=True)\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            axes[1].text(j, i, str(int(output[i, j])), \n",
        "                        ha='center', va='center', color='black', fontsize=14, weight='bold')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\n\ud83d\udcca Max Pooling (2\u00d72, stride=2):')\n",
        "    print('Takes maximum value from each 2\u00d72 region')\n",
        "    print('Reduces spatial dimensions by half')\n",
        "    print('Provides translation invariance')\n",
        "\n",
        "visualize_pooling()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}