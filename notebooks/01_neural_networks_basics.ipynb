{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Neural Networks\n",
    "\n",
    "Welcome to the first chapter! In this notebook, we'll build neural networks from scratch and understand every component in depth.\n",
    "\n",
    "## ðŸ“š Table of Contents\n",
    "1. [What are Neural Networks?](#what-are-neural-networks)\n",
    "2. [The Perceptron: Building Block](#perceptron)\n",
    "3. [Activation Functions](#activation-functions)\n",
    "4. [Multi-Layer Perceptrons (MLPs)](#mlp)\n",
    "5. [Forward Propagation](#forward-prop)\n",
    "6. [Loss Functions](#loss-functions)\n",
    "7. [Backpropagation](#backprop)\n",
    "8. [Optimization Algorithms](#optimization)\n",
    "9. [Training Neural Networks](#training)\n",
    "10. [Biology Application: Gene Expression Classification](#biology-app)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are Neural Networks? <a id=\"what-are-neural-networks\"></a>\n",
    "\n",
    "### The Biological Inspiration\n",
    "\n",
    "Neural networks are inspired by biological neurons in the brain:\n",
    "- **Dendrites** receive signals (inputs)\n",
    "- **Cell body** processes signals\n",
    "- **Axon** sends output to other neurons\n",
    "\n",
    "### Artificial Neurons\n",
    "\n",
    "An artificial neuron:\n",
    "1. Receives multiple inputs: $x_1, x_2, ..., x_n$\n",
    "2. Multiplies each by a weight: $w_1, w_2, ..., w_n$\n",
    "3. Sums them up with a bias: $z = \\sum_{i=1}^{n} w_i x_i + b$\n",
    "4. Applies an activation function: $a = f(z)$\n",
    "\n",
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neuron_diagram():\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = [0.2, 0.5, 0.8]\n",
    "    for i, val in enumerate(inputs):\n",
    "        y_pos = 3 - i\n",
    "        circle = plt.Circle((1, y_pos), 0.15, color='lightblue', ec='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(0.5, y_pos, f'$x_{i+1}$={val}', fontsize=12, ha='right', va='center')\n",
    "        \n",
    "        # Draw connections\n",
    "        ax.plot([1.15, 2.85], [y_pos, 2], 'k-', linewidth=1.5, alpha=0.5)\n",
    "        weight = np.random.uniform(0.5, 1.5)\n",
    "        ax.text(1.8, y_pos + (2-y_pos)/2 + 0.1, f'$w_{i+1}$={weight:.2f}', \n",
    "                fontsize=10, color='red')\n",
    "    \n",
    "    # Neuron\n",
    "    circle = plt.Circle((3, 2), 0.3, color='orange', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(3, 2, 'Î£', fontsize=20, ha='center', va='center', weight='bold')\n",
    "    \n",
    "    # Bias\n",
    "    circle = plt.Circle((3, 0.5), 0.15, color='lightgreen', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(2.5, 0.5, 'bias=$b$', fontsize=10, ha='right', va='center')\n",
    "    ax.plot([3.15, 3], [0.65, 1.7], 'k-', linewidth=1.5, alpha=0.5)\n",
    "    \n",
    "    # Activation\n",
    "    ax.plot([3.3, 4.2], [2, 2], 'k-', linewidth=2)\n",
    "    ax.text(3.75, 2.3, '$z = Î£w_ix_i + b$', fontsize=11, ha='center')\n",
    "    \n",
    "    # Activation function\n",
    "    rect = plt.Rectangle((4.2, 1.7), 0.8, 0.6, color='yellow', ec='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(4.6, 2, '$f(z)$', fontsize=12, ha='center', va='center', weight='bold')\n",
    "    \n",
    "    # Output\n",
    "    ax.plot([5, 5.8], [2, 2], 'k-', linewidth=2)\n",
    "    circle = plt.Circle((6, 2), 0.15, color='lightcoral', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(6.5, 2, 'output', fontsize=12, ha='left', va='center')\n",
    "    \n",
    "    ax.set_xlim(0, 7)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Anatomy of an Artificial Neuron', fontsize=16, weight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_neuron_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Perceptron: Building Block <a id=\"perceptron\"></a>\n",
    "\n",
    "The **perceptron** is the simplest neural network - a single neuron!\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For input vector $\\mathbf{x} = [x_1, x_2, ..., x_n]$ and weights $\\mathbf{w} = [w_1, w_2, ..., w_n]$:\n",
    "\n",
    "$$z = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "$$\\hat{y} = f(z)$$\n",
    "\n",
    "where $f$ is an activation function.\n",
    "\n",
    "### Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"A simple perceptron implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize perceptron.\n",
    "        \n",
    "        Args:\n",
    "            n_inputs: Number of input features\n",
    "            learning_rate: Step size for weight updates\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(n_inputs) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def activation(self, z):\n",
    "        \"\"\"Step function: returns 1 if z >= 0, else 0.\"\"\"\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for input X.\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(z)\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        \"\"\"Train the perceptron using the perceptron learning rule.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_error = 0\n",
    "            \n",
    "            for xi, target in zip(X, y):\n",
    "                # Forward pass\n",
    "                prediction = self.predict(xi.reshape(1, -1))[0]\n",
    "                \n",
    "                # Calculate error\n",
    "                error = target - prediction\n",
    "                total_error += abs(error)\n",
    "                \n",
    "                # Update weights and bias\n",
    "                self.weights += self.lr * error * xi\n",
    "                self.bias += self.lr * error\n",
    "            \n",
    "            errors.append(total_error)\n",
    "        \n",
    "        return errors\n",
    "\n",
    "# Test the perceptron on a simple problem (AND gate)\n",
    "print(\"Testing Perceptron on AND gate:\")\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "perceptron = Perceptron(n_inputs=2, learning_rate=0.1)\n",
    "errors = perceptron.train(X_and, y_and, epochs=100)\n",
    "\n",
    "print(\"\\nTrained weights:\", perceptron.weights)\n",
    "print(\"Trained bias:\", perceptron.bias)\n",
    "print(\"\\nPredictions:\")\n",
    "for xi, yi in zip(X_and, y_and):\n",
    "    pred = perceptron.predict(xi.reshape(1, -1))[0]\n",
    "    print(f\"Input: {xi} | True: {yi} | Predicted: {pred}\")\n",
    "\n",
    "# Plot training error\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(errors, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Total Error', fontsize=12)\n",
    "plt.title('Perceptron Training Error Over Time', fontsize=14, weight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the Perceptron\n",
    "\n",
    "The perceptron can only solve **linearly separable** problems. Let's see what happens with XOR (not linearly separable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try XOR problem (will fail!)\n",
    "print(\"Testing Perceptron on XOR gate (will fail):\")\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "perceptron_xor = Perceptron(n_inputs=2, learning_rate=0.1)\n",
    "errors_xor = perceptron_xor.train(X_xor, y_xor, epochs=1000)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for xi, yi in zip(X_xor, y_xor):\n",
    "    pred = perceptron_xor.predict(xi.reshape(1, -1))[0]\n",
    "    print(f\"Input: {xi} | True: {yi} | Predicted: {pred} {'âœ—' if pred != yi else 'âœ“'}\")\n",
    "\n",
    "# Visualize the problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot AND (linearly separable)\n",
    "axes[0].scatter(X_and[y_and==0, 0], X_and[y_and==0, 1], c='blue', s=200, label='Class 0', edgecolors='black', linewidth=2)\n",
    "axes[0].scatter(X_and[y_and==1, 0], X_and[y_and==1, 1], c='red', s=200, label='Class 1', edgecolors='black', linewidth=2)\n",
    "axes[0].set_xlabel('$x_1$', fontsize=12)\n",
    "axes[0].set_ylabel('$x_2$', fontsize=12)\n",
    "axes[0].set_title('AND Gate - Linearly Separable âœ“', fontsize=13, weight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot XOR (not linearly separable)\n",
    "axes[1].scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=200, label='Class 0', edgecolors='black', linewidth=2)\n",
    "axes[1].scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=200, label='Class 1', edgecolors='black', linewidth=2)\n",
    "axes[1].set_xlabel('$x_1$', fontsize=12)\n",
    "axes[1].set_ylabel('$x_2$', fontsize=12)\n",
    "axes[1].set_title('XOR Gate - NOT Linearly Separable âœ—', fontsize=13, weight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: We need multiple layers to solve XOR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions <a id=\"activation-functions\"></a>\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing networks to learn complex patterns.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "1. **Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "   - Output range: (0, 1)\n",
    "   - Use case: Binary classification output, gates in LSTMs\n",
    "\n",
    "2. **Tanh**: $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "   - Output range: (-1, 1)\n",
    "   - Use case: Hidden layers (zero-centered)\n",
    "\n",
    "3. **ReLU**: $\\text{ReLU}(z) = \\max(0, z)$\n",
    "   - Output range: [0, âˆž)\n",
    "   - Use case: Most common in hidden layers\n",
    "\n",
    "4. **Leaky ReLU**: $\\text{LeakyReLU}(z) = \\max(0.01z, z)$\n",
    "   - Output range: (-âˆž, âˆž)\n",
    "   - Use case: Fixes \"dying ReLU\" problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "# Visualize activation functions\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "activations = [\n",
    "    (sigmoid, 'Sigmoid', r'$\\sigma(z) = \\frac{1}{1 + e^{-z}}$'),\n",
    "    (tanh, 'Tanh', r'$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$'),\n",
    "    (relu, 'ReLU', r'$\\text{ReLU}(z) = \\max(0, z)$'),\n",
    "    (leaky_relu, 'Leaky ReLU', r'$\\text{LeakyReLU}(z) = \\max(0.01z, z)$')\n",
    "]\n",
    "\n",
    "for idx, (func, name, formula) in enumerate(activations):\n",
    "    ax = axes[idx]\n",
    "    y = func(z)\n",
    "    ax.plot(z, y, linewidth=3, color='blue')\n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('Input (z)', fontsize=11)\n",
    "    ax.set_ylabel('Output', fontsize=11)\n",
    "    ax.set_title(f'{name} Activation Function', fontsize=13, weight='bold')\n",
    "    ax.text(0.05, 0.95, formula, transform=ax.transAxes, \n",
    "            fontsize=11, verticalalignment='top', \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Activation Function Properties:\")\n",
    "print(\"\\nSigmoid:\")\n",
    "print(\"  âœ“ Smooth gradient\")\n",
    "print(\"  âœ— Vanishing gradient problem\")\n",
    "print(\"  âœ— Not zero-centered\")\n",
    "print(\"\\nTanh:\")\n",
    "print(\"  âœ“ Zero-centered\")\n",
    "print(\"  âœ— Vanishing gradient problem\")\n",
    "print(\"\\nReLU:\")\n",
    "print(\"  âœ“ No vanishing gradient for positive values\")\n",
    "print(\"  âœ“ Computationally efficient\")\n",
    "print(\"  âœ— Dying ReLU problem (neurons can die)\")\n",
    "print(\"\\nLeaky ReLU:\")\n",
    "print(\"  âœ“ Fixes dying ReLU\")\n",
    "print(\"  âœ“ Small gradient for negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Layer Perceptrons (MLPs) <a id=\"mlp\"></a>\n",
    "\n",
    "An MLP consists of:\n",
    "- **Input layer**: Receives features\n",
    "- **Hidden layer(s)**: Processes information\n",
    "- **Output layer**: Produces predictions\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input (n features) â†’ Hidden Layer 1 (h1 neurons) â†’ Hidden Layer 2 (h2 neurons) â†’ Output (m classes)\n",
    "```\n",
    "\n",
    "Each connection has a weight, and each neuron has a bias.\n",
    "\n",
    "### Building an MLP with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Multi-Layer Perceptron.\n",
    "    \n",
    "    Architecture: Input â†’ Hidden (128) â†’ Hidden (64) â†’ Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size1=128, hidden_size2=64, output_size=2):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # First hidden layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Second hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)  # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Output layer (no activation here - will use softmax in loss)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create a sample network\n",
    "model = SimpleMLP(input_size=10, hidden_size1=128, hidden_size2=64, output_size=2)\n",
    "\n",
    "# Print architecture\n",
    "print(\"MLP Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 5\n",
    "test_input = torch.randn(batch_size, 10)\n",
    "output = model(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving XOR with MLP\n",
    "\n",
    "Now let's solve the XOR problem that defeated the single perceptron!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare XOR data\n",
    "X_xor_tensor = torch.FloatTensor(X_xor)\n",
    "y_xor_tensor = torch.LongTensor(y_xor)\n",
    "\n",
    "# Create small MLP for XOR\n",
    "xor_model = SimpleMLP(input_size=2, hidden_size1=4, hidden_size2=4, output_size=2)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(xor_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = xor_model(X_xor_tensor)\n",
    "    loss = criterion(outputs, y_xor_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "xor_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = xor_model(X_xor_tensor)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "print(\"\\nðŸŽ‰ MLP successfully solves XOR!\")\n",
    "print(\"\\nPredictions:\")\n",
    "for xi, yi, pred in zip(X_xor, y_xor, predicted_classes):\n",
    "    print(f\"Input: {xi} | True: {yi} | Predicted: {pred.item()} {'âœ“' if pred == yi else 'âœ—'}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('MLP Training Loss on XOR Problem', fontsize=14, weight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
